{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess the original dataset"
      ],
      "metadata": {
        "id": "EkEnHfhSQQGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def clean_numeric(x):\n",
        "    \"\"\"Clean numeric values by removing commas and converting to float.\"\"\"\n",
        "    if pd.isna(x):\n",
        "        return np.nan\n",
        "    if isinstance(x, (int, float)):\n",
        "        return float(x)\n",
        "    if isinstance(x, str):\n",
        "        # Remove commas and spaces\n",
        "        x = x.strip().replace(',', '')\n",
        "        # Handle empty strings and dashes\n",
        "        if x in ['', '-', ' -   ', '  -  ']:\n",
        "            return np.nan\n",
        "        try:\n",
        "            return float(x)\n",
        "        except ValueError:\n",
        "            return np.nan\n",
        "    return np.nan\n",
        "\n",
        "def load_data(file_path):\n",
        "    \"\"\"Load the dataset and perform initial analysis.\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Clean column names\n",
        "    df.columns = df.columns.str.strip()\n",
        "\n",
        "    # Convert numeric columns\n",
        "    feature_cols = [col for col in df.columns if col.startswith('Feature')]\n",
        "    target_cols = [col for col in df.columns if col.startswith('Target')]\n",
        "\n",
        "    for col in feature_cols + target_cols:\n",
        "        df[col] = df[col].apply(clean_numeric)\n",
        "\n",
        "    print(f\"Initial shape: {df.shape}\")\n",
        "    print(\"\\nMissing values:\\n\", df.isnull().sum())\n",
        "\n",
        "    # Print data types\n",
        "    print(\"\\nData types:\")\n",
        "    print(df.dtypes)\n",
        "\n",
        "    return df\n",
        "\n",
        "def handle_missing_values(df, n_neighbors=5):\n",
        "    \"\"\"Handle missing values using KNN Imputer.\"\"\"\n",
        "    # Separate features from targets and categorical columns\n",
        "    features = df.filter(regex='Feature\\d+')\n",
        "    targets = df[['Target 1', 'Target 2', 'Target 3']]\n",
        "    categorical = df[['Year', 'Company']]\n",
        "\n",
        "    # Apply KNN imputation to features\n",
        "    imputer = KNNImputer(n_neighbors=n_neighbors)\n",
        "    features_imputed = pd.DataFrame(\n",
        "        imputer.fit_transform(features),\n",
        "        columns=features.columns,\n",
        "        index=features.index\n",
        "    )\n",
        "\n",
        "    # Combine back\n",
        "    df_imputed = pd.concat([categorical, features_imputed, targets], axis=1)\n",
        "    return df_imputed\n",
        "\n",
        "def handle_outliers(df, method = 'iqr'):\n",
        "    \"\"\"Handle outliers using IQR method with winsorization.\"\"\"\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "    for col in numeric_cols:\n",
        "        if col not in ['Year']:  # Skip year column\n",
        "            Q1 = df[col].quantile(0.25)\n",
        "            Q3 = df[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower_bound = Q1 - 1.5 * IQR\n",
        "            upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "            # Winsorization\n",
        "            df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)\n",
        "\n",
        "    return df\n",
        "\n",
        "def normalize_features(df):\n",
        "    \"\"\"Normalize features using StandardScaler.\"\"\"\n",
        "    # Separate features and targets\n",
        "    features = df.filter(regex='Feature\\d+')\n",
        "    targets = df[['Target 1', 'Target 2', 'Target 3']]\n",
        "    categorical = df[['Year', 'Company']]\n",
        "\n",
        "    # Normalize features\n",
        "    scaler = StandardScaler()\n",
        "    features_normalized = pd.DataFrame(\n",
        "        scaler.fit_transform(features),\n",
        "        columns=features.columns,\n",
        "        index=features.index\n",
        "    )\n",
        "\n",
        "    # Combine back\n",
        "    df_normalized = pd.concat([categorical, features_normalized, targets], axis=1)\n",
        "    return df_normalized\n",
        "\n",
        "def analyze_data(df, stage):\n",
        "    \"\"\"Print analysis at different stages.\"\"\"\n",
        "    print(f\"\\n=== Analysis at {stage} ===\")\n",
        "    print(f\"Shape: {df.shape}\")\n",
        "    print(\"\\nSummary statistics for numeric columns:\")\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    print(df[numeric_cols].describe())\n",
        "\n",
        "    if stage == \"Final Stage\":\n",
        "        print(\"\\nTarget variables statistics:\")\n",
        "        print(df[['Target 1', 'Target 2', 'Target 3']].describe())\n",
        "\n",
        "        # Print correlation between features and targets\n",
        "        features = df.filter(regex='Feature\\d+')\n",
        "        targets = df[['Target 1', 'Target 2', 'Target 3']]\n",
        "\n",
        "        print(\"\\nTop 5 features correlated with targets:\")\n",
        "        for target in targets.columns:\n",
        "            correlations = features.corrwith(targets[target]).sort_values(ascending=False)\n",
        "            print(f\"\\n{target}:\")\n",
        "            print(correlations.head())\n",
        "\n",
        "def main():\n",
        "    # Load data\n",
        "    df = load_data('/content/FidelFolio_Dataset.csv')\n",
        "    analyze_data(df, \"Initial Stage\")\n",
        "\n",
        "    # Handle missing values\n",
        "    print(\"\\nHandling missing values...\")\n",
        "    df_clean = handle_missing_values(df)\n",
        "    analyze_data(df_clean, \"After Missing Values Treatment\")\n",
        "\n",
        "    # Handle outliers\n",
        "    print(\"\\nHandling outliers...\")\n",
        "    df_clean = handle_outliers(df_clean)\n",
        "    analyze_data(df_clean, \"After Outlier Treatment\")\n",
        "\n",
        "    # Normalize features\n",
        "    print(\"\\nNormalizing features...\")\n",
        "    df_final = normalize_features(df_clean)\n",
        "    analyze_data(df_final, \"Final Stage\")\n",
        "\n",
        "    # Save processed data\n",
        "    df_final.to_csv('data_KNN.csv', index=False)\n",
        "    print(\"\\nProcessed data saved to 'processed_data.csv'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZ3uOOJVSnP2",
        "outputId": "1ea35235-368a-483d-b43b-f543b659d109"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial shape: (24751, 33)\n",
            "\n",
            "Missing values:\n",
            " Year            0\n",
            "Company         0\n",
            "Feature1     3837\n",
            "Feature2     2752\n",
            "Feature3     1981\n",
            "Feature4      547\n",
            "Feature5      603\n",
            "Feature6      701\n",
            "Feature7      477\n",
            "Feature8        0\n",
            "Feature9     1252\n",
            "Feature10     726\n",
            "Feature11     544\n",
            "Feature12     459\n",
            "Feature13      22\n",
            "Feature14      35\n",
            "Feature15     553\n",
            "Feature16     126\n",
            "Feature17      49\n",
            "Feature18     126\n",
            "Feature19      27\n",
            "Feature20     367\n",
            "Feature21     417\n",
            "Feature22     187\n",
            "Feature23      45\n",
            "Feature24      46\n",
            "Feature25      50\n",
            "Feature26      49\n",
            "Feature27      24\n",
            "Feature28    2032\n",
            "Target 1     1787\n",
            "Target 2     3342\n",
            "Target 3     4866\n",
            "dtype: int64\n",
            "\n",
            "Data types:\n",
            "Year           int64\n",
            "Company       object\n",
            "Feature1     float64\n",
            "Feature2     float64\n",
            "Feature3     float64\n",
            "Feature4     float64\n",
            "Feature5     float64\n",
            "Feature6     float64\n",
            "Feature7     float64\n",
            "Feature8     float64\n",
            "Feature9     float64\n",
            "Feature10    float64\n",
            "Feature11    float64\n",
            "Feature12    float64\n",
            "Feature13    float64\n",
            "Feature14    float64\n",
            "Feature15    float64\n",
            "Feature16    float64\n",
            "Feature17    float64\n",
            "Feature18    float64\n",
            "Feature19    float64\n",
            "Feature20    float64\n",
            "Feature21    float64\n",
            "Feature22    float64\n",
            "Feature23    float64\n",
            "Feature24    float64\n",
            "Feature25    float64\n",
            "Feature26    float64\n",
            "Feature27    float64\n",
            "Feature28    float64\n",
            "Target 1     float64\n",
            "Target 2     float64\n",
            "Target 3     float64\n",
            "dtype: object\n",
            "\n",
            "=== Analysis at Initial Stage ===\n",
            "Shape: (24751, 33)\n",
            "\n",
            "Summary statistics for numeric columns:\n",
            "               Year       Feature1      Feature2      Feature3      Feature4  \\\n",
            "count  24751.000000   20914.000000  21999.000000  22770.000000  2.420400e+04   \n",
            "mean    2012.952972     148.094357     17.857211     15.883015  8.044175e+02   \n",
            "std        7.368602    1985.689454     15.149271     21.162288  1.104263e+05   \n",
            "min     1999.000000       0.010000   -109.440000   -808.950000 -3.000000e+04   \n",
            "25%     2007.000000       7.000000      9.030000      7.210000  5.860000e+00   \n",
            "50%     2014.000000      52.000000     14.830000     14.390000  1.351500e+01   \n",
            "75%     2019.000000     125.750000     23.220000     22.357500  2.751000e+01   \n",
            "max     2024.000000  227428.000000    444.800000    934.770000  1.717728e+07   \n",
            "\n",
            "           Feature5       Feature6      Feature7      Feature8      Feature9  \\\n",
            "count  2.414800e+04   24050.000000  24274.000000  2.475100e+04  23499.000000   \n",
            "mean  -7.074814e+02      -6.263392      0.073667  9.498492e+03      0.272610   \n",
            "std    1.117096e+05    3979.779013     64.429016  4.568186e+04      3.330762   \n",
            "min   -1.735738e+07 -484740.000000  -5634.500000  2.363000e+01     -2.950000   \n",
            "25%    1.417500e+00      -7.435000      0.150000  3.403250e+02     -0.010000   \n",
            "50%    7.410000e+00       3.120000      0.670000  1.027400e+03      0.000000   \n",
            "75%    2.049250e+01      23.130000      1.110000  3.910885e+03      0.080000   \n",
            "max    1.849536e+05  192607.000000   5416.000000  2.014010e+06    201.160000   \n",
            "\n",
            "       ...     Feature22      Feature23      Feature24      Feature25  \\\n",
            "count  ...  24564.000000   24706.000000   24705.000000   24701.000000   \n",
            "mean   ...      5.964409     806.636171    1466.142041    1252.019445   \n",
            "std    ...    290.323524    4096.404868    7543.395905    6993.588813   \n",
            "min    ...  -1018.341463  -42309.000000  -42247.000000  -46404.300000   \n",
            "25%    ...      0.938619      32.290000      51.030000      35.160000   \n",
            "50%    ...      1.979459     108.440000     153.230000     114.710000   \n",
            "75%    ...      4.115079     363.430000     520.570000     404.290000   \n",
            "max    ...  45253.666670  155559.000000  354825.220000  350976.090000   \n",
            "\n",
            "           Feature26     Feature27     Feature28      Target 1      Target 2  \\\n",
            "count   24702.000000  2.472700e+04  2.271900e+04  22964.000000  21409.000000   \n",
            "mean      592.415449  9.612212e+03  1.268924e+04     18.040291     37.758500   \n",
            "std      3337.552197  6.984420e+04  1.095691e+05    130.229198    252.000492   \n",
            "min    -61797.000000 -1.459900e+04  1.000000e-02   -178.490000   -204.900000   \n",
            "25%        17.215000  3.229300e+02  6.593000e+01    -32.010000    -48.670000   \n",
            "50%        73.505000  9.396800e+02  3.020400e+02     -4.165000     -8.250000   \n",
            "75%       264.290000  3.213700e+03  1.378075e+03     35.532500     56.200000   \n",
            "max    104727.000000  5.252252e+06  5.606147e+06  10523.790000  16603.820000   \n",
            "\n",
            "           Target 3  \n",
            "count  19885.000000  \n",
            "mean      68.662595  \n",
            "std      463.939590  \n",
            "min     -344.270000  \n",
            "25%      -64.100000  \n",
            "50%      -11.930000  \n",
            "75%       82.220000  \n",
            "max    29387.270000  \n",
            "\n",
            "[8 rows x 32 columns]\n",
            "\n",
            "Handling missing values...\n",
            "\n",
            "=== Analysis at After Missing Values Treatment ===\n",
            "Shape: (24751, 33)\n",
            "\n",
            "Summary statistics for numeric columns:\n",
            "               Year       Feature1      Feature2      Feature3      Feature4  \\\n",
            "count  24751.000000   24751.000000  24751.000000  24751.000000  2.475100e+04   \n",
            "mean    2012.952972     160.548114     16.839485     15.020538  7.910287e+02   \n",
            "std        7.368602    1867.505148     14.752023     21.037937  1.091994e+05   \n",
            "min     1999.000000       0.010000   -109.440000   -808.950000 -3.000000e+04   \n",
            "25%     2007.000000       8.000000      8.080000      6.387000  5.960000e+00   \n",
            "50%     2014.000000      48.000000     13.690000     13.740000  1.375000e+01   \n",
            "75%     2019.000000     127.000000     22.105000     21.780000  2.806500e+01   \n",
            "max     2024.000000  227428.000000    444.800000    934.770000  1.717728e+07   \n",
            "\n",
            "           Feature5       Feature6      Feature7      Feature8      Feature9  \\\n",
            "count  2.475100e+04   24751.000000  24751.000000  2.475100e+04  24751.000000   \n",
            "mean  -6.893529e+02     -10.312603      0.054045  9.498492e+03      0.309361   \n",
            "std    1.103407e+05    3933.086508     63.883200  4.568186e+04      3.330929   \n",
            "min   -1.735738e+07 -484740.000000  -5634.500000  2.363000e+01     -2.950000   \n",
            "25%    1.370000e+00      -7.589000      0.150000  3.403250e+02     -0.010000   \n",
            "50%    7.360000e+00       3.010000      0.670000  1.027400e+03      0.010000   \n",
            "75%    2.053100e+01      23.075000      1.110000  3.910885e+03      0.090000   \n",
            "max    1.849536e+05  192607.000000   5416.000000  2.014010e+06    201.160000   \n",
            "\n",
            "       ...     Feature22      Feature23      Feature24      Feature25  \\\n",
            "count  ...  24751.000000   24751.000000   24751.000000   24751.000000   \n",
            "mean   ...      5.964276     805.595204    1464.057913    1250.024369   \n",
            "std    ...    289.226008    4092.875727    7536.628361    6986.733892   \n",
            "min    ...  -1018.341463  -42309.000000  -42247.000000  -46404.300000   \n",
            "25%    ...      0.941272      32.140000      50.845000      35.035000   \n",
            "50%    ...      1.984935     108.060000     152.840000     114.450000   \n",
            "75%    ...      4.126900     363.055000     520.120000     403.725000   \n",
            "max    ...  45253.666670  155559.000000  354825.220000  350976.090000   \n",
            "\n",
            "           Feature26     Feature27     Feature28      Target 1      Target 2  \\\n",
            "count   24751.000000  2.475100e+04  2.475100e+04  22964.000000  21409.000000   \n",
            "mean      591.561734  9.604310e+03  1.209169e+04     18.040291     37.758500   \n",
            "std      3334.405392  6.981085e+04  1.127777e+05    130.229198    252.000492   \n",
            "min    -61797.000000 -1.459900e+04  1.000000e-02   -178.490000   -204.900000   \n",
            "25%        17.125000  3.227000e+02  5.265500e+01    -32.010000    -48.670000   \n",
            "50%        73.240000  9.390400e+02  2.520480e+02     -4.165000     -8.250000   \n",
            "75%       263.740000  3.208920e+03  1.202940e+03     35.532500     56.200000   \n",
            "max    104727.000000  5.252252e+06  5.606147e+06  10523.790000  16603.820000   \n",
            "\n",
            "           Target 3  \n",
            "count  19885.000000  \n",
            "mean      68.662595  \n",
            "std      463.939590  \n",
            "min     -344.270000  \n",
            "25%      -64.100000  \n",
            "50%      -11.930000  \n",
            "75%       82.220000  \n",
            "max    29387.270000  \n",
            "\n",
            "[8 rows x 32 columns]\n",
            "\n",
            "Handling outliers...\n",
            "\n",
            "=== Analysis at After Outlier Treatment ===\n",
            "Shape: (24751, 33)\n",
            "\n",
            "Summary statistics for numeric columns:\n",
            "               Year      Feature1      Feature2      Feature3      Feature4  \\\n",
            "count  24751.000000  24751.000000  24751.000000  24751.000000  24751.000000   \n",
            "mean    2012.952972     85.777966     16.020924     14.705248     18.327143   \n",
            "std        7.368602     96.876766     11.234448     12.740785     21.172204   \n",
            "min     1999.000000      0.010000    -12.957500    -16.702500    -27.197500   \n",
            "25%     2007.000000      8.000000      8.080000      6.387000      5.960000   \n",
            "50%     2014.000000     48.000000     13.690000     13.740000     13.750000   \n",
            "75%     2019.000000    127.000000     22.105000     21.780000     28.065000   \n",
            "max     2024.000000    305.500000     43.142500     44.869500     61.222500   \n",
            "\n",
            "           Feature5      Feature6      Feature7      Feature8      Feature9  \\\n",
            "count  24751.000000  24751.000000  24751.000000  24751.000000  24751.000000   \n",
            "mean      10.717976      7.417278      0.625094   2702.908342      0.039536   \n",
            "std       19.851057     33.173426      0.935930   3276.084376      0.111988   \n",
            "min      -27.371500    -53.585000     -1.290000     23.630000     -0.160000   \n",
            "25%        1.370000     -7.589000      0.150000    340.325000     -0.010000   \n",
            "50%        7.360000      3.010000      0.670000   1027.400000      0.010000   \n",
            "75%       20.531000     23.075000      1.110000   3910.885000      0.090000   \n",
            "max       49.272500     69.071000      2.550000   9266.725000      0.240000   \n",
            "\n",
            "       ...     Feature22     Feature23     Feature24     Feature25  \\\n",
            "count  ...  24751.000000  24751.000000  24751.000000  24751.000000   \n",
            "mean   ...      2.881936    237.332722    353.915863    270.616331   \n",
            "std    ...      2.726444    316.179277    434.637839    347.712575   \n",
            "min    ...     -3.837170   -464.232500   -653.067500   -518.000000   \n",
            "25%    ...      0.941272     32.140000     50.845000     35.035000   \n",
            "50%    ...      1.984935    108.060000    152.840000    114.450000   \n",
            "75%    ...      4.126900    363.055000    520.120000    403.725000   \n",
            "max    ...      8.905342    859.427500   1224.032500    956.760000   \n",
            "\n",
            "          Feature26     Feature27     Feature28      Target 1      Target 2  \\\n",
            "count  24751.000000  24751.000000  24751.000000  22964.000000  21409.000000   \n",
            "mean     164.578099   2227.701566    818.168845      7.377434     14.374549   \n",
            "std      244.969802   2622.897059   1068.326383     57.851592     89.547556   \n",
            "min     -352.797500  -4006.630000      0.010000   -133.323750   -204.900000   \n",
            "25%       17.125000    322.700000     52.655000    -32.010000    -48.670000   \n",
            "50%       73.240000    939.040000    252.048000     -4.165000     -8.250000   \n",
            "75%      263.740000   3208.920000   1202.940000     35.532500     56.200000   \n",
            "max      633.662500   7538.250000   2928.367500    136.846250    213.505000   \n",
            "\n",
            "           Target 3  \n",
            "count  19885.000000  \n",
            "mean      24.241587  \n",
            "std      125.687736  \n",
            "min     -283.580000  \n",
            "25%      -64.100000  \n",
            "50%      -11.930000  \n",
            "75%       82.220000  \n",
            "max      301.700000  \n",
            "\n",
            "[8 rows x 32 columns]\n",
            "\n",
            "Normalizing features...\n",
            "\n",
            "=== Analysis at Final Stage ===\n",
            "Shape: (24751, 33)\n",
            "\n",
            "Summary statistics for numeric columns:\n",
            "               Year      Feature1      Feature2      Feature3      Feature4  \\\n",
            "count  24751.000000  2.475100e+04  2.475100e+04  2.475100e+04  2.475100e+04   \n",
            "mean    2012.952972  1.056441e-16 -2.773875e-16 -1.827241e-16 -2.526272e-17   \n",
            "std        7.368602  1.000020e+00  1.000020e+00  1.000020e+00  1.000020e+00   \n",
            "min     1999.000000 -8.853485e-01 -2.579478e+00 -2.465184e+00 -2.150251e+00   \n",
            "25%     2007.000000 -8.028709e-01 -7.068513e-01 -6.528967e-01 -5.841335e-01   \n",
            "50%     2014.000000 -3.899669e-01 -2.074842e-01 -7.576199e-02 -2.161908e-01   \n",
            "75%     2019.000000  4.255186e-01  5.415664e-01  5.552951e-01  4.599452e-01   \n",
            "max     2024.000000  2.268103e+00  2.414193e+00  2.367583e+00  2.026063e+00   \n",
            "\n",
            "           Feature5      Feature6      Feature7      Feature8      Feature9  \\\n",
            "count  2.475100e+04  2.475100e+04  2.475100e+04  2.475100e+04  2.475100e+04   \n",
            "mean   7.865893e-17  1.069359e-17 -1.550212e-17 -1.228687e-16  3.444916e-17   \n",
            "std    1.000020e+00  1.000020e+00  1.000020e+00  1.000020e+00  1.000020e+00   \n",
            "min   -1.918802e+00 -1.838927e+00 -2.046236e+00 -8.178460e-01 -1.781801e+00   \n",
            "25%   -4.709152e-01 -4.523676e-01 -5.076276e-01 -7.211753e-01 -4.423435e-01   \n",
            "50%   -1.691620e-01 -1.328584e-01  4.798079e-02 -5.114466e-01 -2.637491e-01   \n",
            "75%    4.943426e-01  4.720055e-01  5.181110e-01  3.687332e-01  4.506285e-01   \n",
            "max    1.942229e+00  1.858565e+00  2.056719e+00  2.003596e+00  1.790086e+00   \n",
            "\n",
            "       ...     Feature22     Feature23     Feature24     Feature25  \\\n",
            "count  ...  2.475100e+04  2.475100e+04  2.475100e+04  2.475100e+04   \n",
            "mean   ...  8.497461e-17  3.444916e-18 -3.100425e-17 -1.768390e-16   \n",
            "std    ...  1.000020e+00  1.000020e+00  1.000020e+00  1.000020e+00   \n",
            "min    ... -2.464471e+00 -2.218929e+00 -2.316880e+00 -2.268058e+00   \n",
            "25%    ... -7.118072e-01 -6.489890e-01 -6.973092e-01 -6.775311e-01   \n",
            "50%    ... -3.290067e-01 -4.088672e-01 -4.626379e-01 -4.491338e-01   \n",
            "75%    ...  4.566351e-01  3.976378e-01  3.824046e-01  3.828201e-01   \n",
            "max    ...  2.209299e+00  1.967578e+00  2.001975e+00  1.973347e+00   \n",
            "\n",
            "          Feature26     Feature27     Feature28      Target 1      Target 2  \\\n",
            "count  2.475100e+04  2.475100e+04  2.475100e+04  22964.000000  21409.000000   \n",
            "mean  -3.100425e-17 -4.306146e-17  6.143434e-17      7.377434     14.374549   \n",
            "std    1.000020e+00  1.000020e+00  1.000020e+00     57.851592     89.547556   \n",
            "min   -2.112040e+00 -2.376936e+00 -7.658478e-01   -133.323750   -204.900000   \n",
            "25%   -6.019357e-01 -7.263114e-01 -7.165688e-01    -32.010000    -48.670000   \n",
            "50%   -3.728621e-01 -4.913222e-01 -5.299245e-01     -4.165000     -8.250000   \n",
            "75%    4.048005e-01  3.741048e-01  3.601698e-01     35.532500     56.200000   \n",
            "max    1.914905e+00  2.024729e+00  1.975278e+00    136.846250    213.505000   \n",
            "\n",
            "           Target 3  \n",
            "count  19885.000000  \n",
            "mean      24.241587  \n",
            "std      125.687736  \n",
            "min     -283.580000  \n",
            "25%      -64.100000  \n",
            "50%      -11.930000  \n",
            "75%       82.220000  \n",
            "max      301.700000  \n",
            "\n",
            "[8 rows x 32 columns]\n",
            "\n",
            "Target variables statistics:\n",
            "           Target 1      Target 2      Target 3\n",
            "count  22964.000000  21409.000000  19885.000000\n",
            "mean       7.377434     14.374549     24.241587\n",
            "std       57.851592     89.547556    125.687736\n",
            "min     -133.323750   -204.900000   -283.580000\n",
            "25%      -32.010000    -48.670000    -64.100000\n",
            "50%       -4.165000     -8.250000    -11.930000\n",
            "75%       35.532500     56.200000     82.220000\n",
            "max      136.846250    213.505000    301.700000\n",
            "\n",
            "Top 5 features correlated with targets:\n",
            "\n",
            "Target 1:\n",
            "Feature3     0.057596\n",
            "Feature7     0.056424\n",
            "Feature15    0.051312\n",
            "Feature16    0.040807\n",
            "Feature12    0.039790\n",
            "dtype: float64\n",
            "\n",
            "Target 2:\n",
            "Feature7     0.067546\n",
            "Feature15    0.060077\n",
            "Feature16    0.034423\n",
            "Feature18    0.032475\n",
            "Feature12    0.031935\n",
            "dtype: float64\n",
            "\n",
            "Target 3:\n",
            "Feature7     0.069308\n",
            "Feature15    0.066677\n",
            "Feature1     0.046942\n",
            "Feature12    0.033871\n",
            "Feature16    0.030872\n",
            "dtype: float64\n",
            "\n",
            "Processed data saved to 'processed_data.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/data_KNN.csv')"
      ],
      "metadata": {
        "id": "CnngH-k9VZoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "y9CUVNQhV_G_",
        "outputId": "7ccf0389-37a9-46d8-8ab4-261e22d0e71f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Year         24751\n",
              "Company      24751\n",
              "Feature1     24751\n",
              "Feature2     24751\n",
              "Feature3     24751\n",
              "Feature4     24751\n",
              "Feature5     24751\n",
              "Feature6     24751\n",
              "Feature7     24751\n",
              "Feature8     24751\n",
              "Feature9     24751\n",
              "Feature10    24751\n",
              "Feature11    24751\n",
              "Feature12    24751\n",
              "Feature13    24751\n",
              "Feature14    24751\n",
              "Feature15    24751\n",
              "Feature16    24751\n",
              "Feature17    24751\n",
              "Feature18    24751\n",
              "Feature19    24751\n",
              "Feature20    24751\n",
              "Feature21    24751\n",
              "Feature22    24751\n",
              "Feature23    24751\n",
              "Feature24    24751\n",
              "Feature25    24751\n",
              "Feature26    24751\n",
              "Feature27    24751\n",
              "Feature28    24751\n",
              "Target 1     22964\n",
              "Target 2     21409\n",
              "Target 3     19885\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Year</th>\n",
              "      <td>24751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Company</th>\n",
              "      <td>24751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Feature1</th>\n",
              "      <td>24751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Feature2</th>\n",
              "      <td>24751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Feature3</th>\n",
              "      <td>24751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Feature4</th>\n",
              "      <td>24751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Feature5</th>\n",
              "      <td>24751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Feature6</th>\n",
              "      <td>24751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Feature7</th>\n",
              "      <td>24751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Feature8</th>\n",
              "      <td>24751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Feature9</th>\n",
              "      <td>24751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Feature10</th>\n",
              "      <td>24751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Feature11</th>\n",
              "      <td>24751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Feature12</th>\n",
              "      <td>24751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Feature13</th>\n",
              "      <td>24751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Feature14</th>\n",
              "      <td>24751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Feature15</th>\n",
              "      <td>24751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Feature16</th>\n",
              "      <td>24751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Feature17</th>\n",
              "      <td>24751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Feature18</th>\n",
              "      <td>24751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Feature19</th>\n",
              "      <td>24751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Feature20</th>\n",
              "      <td>24751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Feature21</th>\n",
              "      <td>24751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Feature22</th>\n",
              "      <td>24751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Feature23</th>\n",
              "      <td>24751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Feature24</th>\n",
              "      <td>24751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Feature25</th>\n",
              "      <td>24751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Feature26</th>\n",
              "      <td>24751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Feature27</th>\n",
              "      <td>24751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Feature28</th>\n",
              "      <td>24751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Target 1</th>\n",
              "      <td>22964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Target 2</th>\n",
              "      <td>21409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Target 3</th>\n",
              "      <td>19885</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['Target 1'].isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_TON-kkVom0",
        "outputId": "69e7aff8-4362-439a-b982-1a00ddc6cf70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int64(1787)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['Target 2'].isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyZQR1WOV5zE",
        "outputId": "ec7cf1c7-5bfb-46ec-8229-5080bdeae55b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int64(3342)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['Target 3'].isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTCLjmH6V7pP",
        "outputId": "e3a7248a-25e1-4faa-e9d5-b2a86089b8d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int64(4866)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df.drop(['Target 2', 'Target 3'], axis=1).dropna()\n",
        "df2 = df.drop(['Target 1', 'Target 3'], axis=1).dropna()\n",
        "df3 = df.drop(['Target 1', 'Target 2'], axis=1).dropna()"
      ],
      "metadata": {
        "id": "27KC21qSV-GZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNW2oIQsXoNk",
        "outputId": "c619a8a4-09a7-4193-b47c-b67278138d64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22964, 31)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tFOWcFUXzBN",
        "outputId": "73941450-39f1-4479-c7fb-b7b13f131456"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21409, 31)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df3.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFlx5SARX8ci",
        "outputId": "a4e9a1b8-79c2-46a9-e7f8-3534a8f372a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19885, 31)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, Input, MultiHeadAttention, LayerNormalization, Flatten, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import os\n",
        "import shap  # For SHAP values calculation\n",
        "from sklearn.inspection import permutation_importance  # For feature importance"
      ],
      "metadata": {
        "id": "ezYmcr9Eqygi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Target 1"
      ],
      "metadata": {
        "id": "cwHGb8S4rn63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Create directories for saving models and results\n",
        "os.makedirs('models', exist_ok=True)\n",
        "os.makedirs('results', exist_ok=True)\n",
        "os.makedirs('plots', exist_ok=True)\n",
        "os.makedirs('interpretability', exist_ok=True)  # For interpretability results\n",
        "\n",
        "# ================= MODEL INTERPRETABILITY FUNCTIONS =================\n",
        "# Analyze feature importance using a permutation approach\n",
        "def analyze_feature_importance_inline(model_type, target_name, X_test, y_test, feature_names, model=None):\n",
        "    \"\"\"\n",
        "    Analyze and visualize feature importance directly without loading saved models\n",
        "    \"\"\"\n",
        "    print(f\"\\nAnalyzing feature importance for {model_type} on {target_name}...\")\n",
        "\n",
        "    if model is None:\n",
        "        print(f\"No model provided for {model_type} on {target_name}, skipping analysis\")\n",
        "        return None\n",
        "\n",
        "    # Create baseline prediction\n",
        "    baseline_pred = model.predict(X_test, verbose=0).flatten()\n",
        "    baseline_mse = mean_squared_error(y_test, baseline_pred)\n",
        "\n",
        "    # Calculate feature importance via permutation\n",
        "    feature_importances = []\n",
        "\n",
        "    # For sequence data, analyze each feature\n",
        "    seq_length, n_features = X_test.shape[1], X_test.shape[2]\n",
        "\n",
        "    for feat_idx in range(n_features):\n",
        "        # Copy the test data\n",
        "        X_permuted = X_test.copy()\n",
        "\n",
        "        # Permute this feature across all time steps\n",
        "        for t in range(seq_length):\n",
        "            X_permuted[:, t, feat_idx] = np.random.permutation(X_permuted[:, t, feat_idx])\n",
        "\n",
        "        # Make predictions with permuted feature\n",
        "        perm_pred = model.predict(X_permuted, verbose=0).flatten()\n",
        "        perm_mse = mean_squared_error(y_test, perm_pred)\n",
        "\n",
        "        # Importance is the increase in error when feature is permuted\n",
        "        importance = perm_mse - baseline_mse\n",
        "        feature_importances.append(importance)\n",
        "\n",
        "    # Create DataFrame with feature importances\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': feature_importances\n",
        "    })\n",
        "    importance_df = importance_df.sort_values('Importance', ascending=False)\n",
        "\n",
        "    # Save to CSV\n",
        "    importance_df.to_csv(f\"interpretability/{model_type}{target_name.replace(' ', '')}_importance.csv\", index=False)\n",
        "\n",
        "    # Visualize top features\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    top_features = importance_df.head(10)\n",
        "    sns.barplot(x='Importance', y='Feature', data=top_features, palette='viridis')\n",
        "    plt.title(f\"Top 10 Important Features for {target_name} ({model_type.upper()})\")\n",
        "    plt.xlabel('Permutation Importance (higher = more important)')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"interpretability/{model_type}{target_name.replace(' ', '')}_importance.png\")\n",
        "    plt.close()\n",
        "\n",
        "    return importance_df\n",
        "\n",
        "# Visualize prediction distribution and confidence\n",
        "def visualize_prediction_confidence_inline(model_type, target_name, X_test, y_test, model=None):\n",
        "    \"\"\"\n",
        "    Visualize model predictions, errors, and confidence directly without loading saved models\n",
        "    \"\"\"\n",
        "    print(f\"\\nVisualizing prediction confidence for {model_type} on {target_name}...\")\n",
        "\n",
        "    if model is None:\n",
        "        print(f\"No model provided for {model_type} on {target_name}, skipping visualization\")\n",
        "        return\n",
        "\n",
        "    # Generate predictions\n",
        "    y_pred = model.predict(X_test, verbose=0).flatten()\n",
        "\n",
        "    # Calculate absolute error for each prediction\n",
        "    abs_errors = np.abs(y_test - y_pred)\n",
        "\n",
        "    # Create dataframe for visualization\n",
        "    pred_df = pd.DataFrame({\n",
        "        'Actual': y_test,\n",
        "        'Predicted': y_pred,\n",
        "        'AbsError': abs_errors\n",
        "    })\n",
        "\n",
        "    # Create scatter plot with error as color\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    scatter = plt.scatter(pred_df['Actual'], pred_df['Predicted'],\n",
        "                         c=pred_df['AbsError'], cmap='viridis', alpha=0.7)\n",
        "\n",
        "    # Add perfect prediction line\n",
        "    min_val = min(pred_df['Actual'].min(), pred_df['Predicted'].min())\n",
        "    max_val = max(pred_df['Actual'].max(), pred_df['Predicted'].max())\n",
        "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', label='Perfect Prediction')\n",
        "\n",
        "    plt.colorbar(scatter, label='Absolute Error')\n",
        "    plt.xlabel('Actual Values')\n",
        "    plt.ylabel('Predicted Values')\n",
        "    plt.title(f'Prediction Confidence - {model_type.upper()} for {target_name}')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"interpretability/{model_type}{target_name.replace(' ', '')}_prediction_confidence.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # Create error distribution plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.histplot(pred_df['AbsError'], kde=True)\n",
        "    plt.xlabel('Absolute Error')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title(f'Error Distribution - {model_type.upper()} for {target_name}')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"interpretability/{model_type}{target_name.replace(' ', '')}_error_distribution.png\")\n",
        "    plt.close()\n",
        "\n",
        "# Create a function to compare importances across targets\n",
        "def compare_importances(importance_data, model_type):\n",
        "    \"\"\"Compare feature importance across different targets\"\"\"\n",
        "    # If we have data for all targets, create comparison\n",
        "    if len(importance_data) >= 2:  # Need at least 2 targets to compare\n",
        "        # Get top 5 features from each target\n",
        "        all_top_features = []\n",
        "        for target, imp_df in importance_data.items():\n",
        "            all_top_features.extend(imp_df.head(5)['Feature'].tolist())\n",
        "\n",
        "        # Create unique list\n",
        "        all_top_features = list(set(all_top_features))\n",
        "\n",
        "        # Create comparison dataframe\n",
        "        comparison_df = pd.DataFrame({'Feature': all_top_features})\n",
        "\n",
        "        # Add importance for each target\n",
        "        for target, imp_df in importance_data.items():\n",
        "            # Get importance values for these features\n",
        "            target_df = imp_df[imp_df['Feature'].isin(all_top_features)]\n",
        "            # Create a mapping from feature to importance\n",
        "            importance_map = dict(zip(target_df['Feature'], target_df['Importance']))\n",
        "\n",
        "            # Add to comparison df with the target name as column\n",
        "            comparison_df[target] = comparison_df['Feature'].map(importance_map).fillna(0)\n",
        "\n",
        "        # Save to CSV\n",
        "        comparison_df.to_csv(f\"interpretability/{model_type}_feature_importance_comparison.csv\", index=False)\n",
        "\n",
        "        # Create heatmap visualization\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        heatmap_df = comparison_df.set_index('Feature')\n",
        "        sns.heatmap(heatmap_df, annot=True, cmap='viridis', fmt='.3f')\n",
        "        plt.title(f'Feature Importance Comparison Across Targets - {model_type.upper()}')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"interpretability/{model_type}_feature_importance_heatmap.png\")\n",
        "        plt.close()\n",
        "\n",
        "# # Load the preprocessed dataset\n",
        "# data = pd.read_csv('processed_data.csv')\n",
        "# print(f\"Dataset loaded with shape: {data.shape}\")\n",
        "\n",
        "data = df1\n",
        "\n",
        "# # Define feature and target columns\n",
        "feature_columns = [f'Feature{i}' for i in range(1, 29)]\n",
        "target_columns = ['Target 1']\n",
        "\n",
        "# Sort data by Year and Company\n",
        "data = data.sort_values(['Company', 'Year'])\n",
        "\n",
        "# Prepare sequences for time-series modeling\n",
        "def prepare_sequences(df, feature_cols, target_cols, seq_length=3):\n",
        "    \"\"\"Prepare time series sequences for each company\"\"\"\n",
        "    companies = df['Company'].unique()\n",
        "    X_sequences = []\n",
        "    y_dict = {target: [] for target in target_cols}\n",
        "    companies_included = []\n",
        "    years_included = []\n",
        "\n",
        "    for company in companies:\n",
        "        company_data = df[df['Company'] == company]\n",
        "        if len(company_data) >= seq_length + 1:  # +1 because we need at least one target\n",
        "            company_X = company_data[feature_cols].values\n",
        "            company_years = company_data['Year'].values\n",
        "\n",
        "            for i in range(len(company_data) - seq_length):\n",
        "                X_sequences.append(company_X[i:i+seq_length])\n",
        "                companies_included.append(company)\n",
        "                years_included.append(company_years[i+seq_length-1])\n",
        "\n",
        "                for target in target_cols:\n",
        "                    y_dict[target].append(company_data[target].values[i+seq_length])\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    X_sequences = np.array(X_sequences)\n",
        "    for target in target_cols:\n",
        "        y_dict[target] = np.array(y_dict[target])\n",
        "\n",
        "    # Create tracking DataFrame for analysis\n",
        "    tracking_df = pd.DataFrame({\n",
        "        'Company': companies_included,\n",
        "        'Year': years_included\n",
        "    })\n",
        "\n",
        "    return X_sequences, y_dict, tracking_df\n",
        "\n",
        "# Create sequences\n",
        "X_sequences, y_dict, tracking_df = prepare_sequences(data, feature_columns, target_columns, seq_length=3)\n",
        "print(f\"Created {len(X_sequences)} sequences with shape {X_sequences.shape}\")\n",
        "\n",
        "# Print year distribution\n",
        "year_distribution = tracking_df['Year'].value_counts().sort_index()\n",
        "print(f\"Year distribution in sequences:\")\n",
        "print(year_distribution)\n",
        "\n",
        "# Implement strict non-overlapping time-based CV\n",
        "def strict_time_cv_split(X, y_dict, tracking_df, n_splits=5):\n",
        "    \"\"\"\n",
        "    Create strictly non-overlapping time-based CV splits\n",
        "    Ensures no year appears in both training and testing sets\n",
        "    \"\"\"\n",
        "    years = sorted(tracking_df['Year'].unique())\n",
        "    total_years = len(years)\n",
        "\n",
        "    # Calculate approximate number of years for each split\n",
        "    years_per_split = total_years // n_splits\n",
        "\n",
        "    cv_splits = []\n",
        "    for i in range(n_splits):\n",
        "        # Calculate year boundaries (no overlap)\n",
        "        if i < n_splits - 1:\n",
        "            train_end_year = years[min((i+1) * years_per_split - 1, total_years-1)]\n",
        "            test_start_year = years[min((i+1) * years_per_split, total_years-1)]\n",
        "            test_end_year = years[min((i+2) * years_per_split - 1, total_years-1)]\n",
        "        else:\n",
        "            # Last fold uses all remaining years for testing\n",
        "            train_end_year = years[min((i+1) * years_per_split - 1, total_years-1)]\n",
        "            test_start_year = years[min((i+1) * years_per_split, total_years-1)]\n",
        "            test_end_year = years[-1]\n",
        "\n",
        "        # Get indices for this split\n",
        "        train_indices = tracking_df[tracking_df['Year'] <= train_end_year].index\n",
        "        test_indices = tracking_df[(tracking_df['Year'] >= test_start_year) &\n",
        "                                   (tracking_df['Year'] <= test_end_year)].index\n",
        "\n",
        "        # Create the train/test split\n",
        "        X_train = X[train_indices]\n",
        "        X_test = X[test_indices]\n",
        "        y_train = {target: y_dict[target][train_indices] for target in y_dict}\n",
        "        y_test = {target: y_dict[target][test_indices] for target in y_dict}\n",
        "\n",
        "        cv_splits.append((X_train, y_train, X_test, y_test))\n",
        "\n",
        "        # Print information about this fold\n",
        "        train_years = tracking_df.iloc[train_indices]['Year'].unique()\n",
        "        test_years = tracking_df.iloc[test_indices]['Year'].unique()\n",
        "        print(f\"Fold {i+1}: Train years: {min(train_years)}-{max(train_years)}, \"\n",
        "              f\"Test years: {min(test_years)}-{max(test_years)}, \"\n",
        "              f\"Train size: {len(train_indices)}, Test size: {len(test_indices)}\")\n",
        "\n",
        "    return cv_splits\n",
        "\n",
        "# Create CV splits with strict time-based boundaries\n",
        "print(\"\\nCreating strict time-based cross-validation splits (no overlapping years)...\")\n",
        "cv_splits = strict_time_cv_split(X_sequences, y_dict, tracking_df, n_splits=5)\n",
        "\n",
        "# Model building functions\n",
        "def build_mlp_model(input_shape, target_name):\n",
        "    \"\"\"Build a Multi-Layer Perceptron model\"\"\"\n",
        "    input_layer = Input(shape=input_shape)\n",
        "    x = Flatten()(input_layer)\n",
        "\n",
        "    # MLP layers\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    output = Dense(1, activation='linear')(x)\n",
        "\n",
        "    # Use safe name without spaces\n",
        "    model = Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def build_lstm_model(input_shape, target_name):\n",
        "    \"\"\"Build an LSTM model\"\"\"\n",
        "    input_layer = Input(shape=input_shape)\n",
        "\n",
        "    # LSTM layers\n",
        "    x = LSTM(32, return_sequences=True)(input_layer)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    x = LSTM(16)(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    output = Dense(1, activation='linear')(x)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def build_transformer_model(input_shape, target_name):\n",
        "    \"\"\"Build a simple Transformer model\"\"\"\n",
        "    input_layer = Input(shape=input_shape)\n",
        "\n",
        "    # Self-attention\n",
        "    x = input_layer\n",
        "\n",
        "    # Transformer block\n",
        "    attn = MultiHeadAttention(num_heads=4, key_dim=8)(x, x)\n",
        "    x = LayerNormalization(epsilon=1e-6)(x + attn)\n",
        "\n",
        "    # Feed-forward network\n",
        "    ffn = Dense(32, activation='relu')(x)\n",
        "    ffn = Dense(input_shape[-1])(ffn)\n",
        "    x = LayerNormalization(epsilon=1e-6)(x + ffn)\n",
        "\n",
        "    # Flatten for final dense layers\n",
        "    x = Flatten()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    output = Dense(1, activation='linear')(x)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Train and evaluate with CV\n",
        "def train_and_evaluate_cv(model_type, cv_splits, input_shape, target_name):\n",
        "    \"\"\"Train and evaluate a model using cross-validation for a specific target\"\"\"\n",
        "    all_rmse = []\n",
        "    all_models = []\n",
        "\n",
        "    for fold_idx, (X_train, y_train_dict, X_test, y_test_dict) in enumerate(cv_splits):\n",
        "        # Get target-specific data\n",
        "        y_train = y_train_dict[target_name]\n",
        "        y_test = y_test_dict[target_name]\n",
        "\n",
        "        print(f\"\\nTraining {model_type} for {target_name} - Fold {fold_idx+1}/{len(cv_splits)}\")\n",
        "\n",
        "        # Build model based on type\n",
        "        if model_type == 'mlp':\n",
        "            model = build_mlp_model(input_shape, target_name)\n",
        "        elif model_type == 'lstm':\n",
        "            model = build_lstm_model(input_shape, target_name)\n",
        "        elif model_type == 'transformer':\n",
        "            model = build_transformer_model(input_shape, target_name)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
        "\n",
        "        callbacks = [\n",
        "            EarlyStopping(patience=5, restore_best_weights=True),\n",
        "            ReduceLROnPlateau(factor=0.5, patience=3, min_lr=1e-6)\n",
        "        ]\n",
        "\n",
        "        # Train model\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_split=0.2,\n",
        "            epochs=10,  # Limited epochs for demo\n",
        "            batch_size=32,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Evaluate on test data\n",
        "        y_pred = model.predict(X_test).flatten()\n",
        "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "        all_rmse.append(rmse)\n",
        "        all_models.append(model)\n",
        "\n",
        "        print(f\"{model_type.upper()} - {target_name} - Fold {fold_idx+1} Test RMSE: {rmse:.4f}\")\n",
        "\n",
        "        # Plot loss curves for this fold\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(history.history['loss'], label='Training Loss')\n",
        "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "        plt.title(f'{model_type.upper()} Loss for {target_name} - Fold {fold_idx+1}')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss (MSE)')\n",
        "        plt.legend()\n",
        "        plt.savefig(f\"plots/{model_type}{target_name.replace(' ', '')}_fold{fold_idx+1}_loss.png\")\n",
        "        plt.close()\n",
        "\n",
        "        # Save predictions for this fold\n",
        "        results_df = pd.DataFrame({\n",
        "            'Actual': y_test,\n",
        "            'Predicted': y_pred,\n",
        "            'Error': y_test - y_pred\n",
        "        })\n",
        "        results_df.to_csv(f\"results/{model_type}{target_name.replace(' ', '')}_fold{fold_idx+1}_predictions.csv\", index=False)\n",
        "\n",
        "        # For the last fold, perform model interpretability\n",
        "        if fold_idx == len(cv_splits) - 1:\n",
        "            # Analyze feature importance\n",
        "            analyze_feature_importance_inline(\n",
        "                model_type=model_type,\n",
        "                target_name=target_name,\n",
        "                X_test=X_test,\n",
        "                y_test=y_test,\n",
        "                feature_names=feature_columns,\n",
        "                model=model\n",
        "            )\n",
        "\n",
        "            # Visualize prediction confidence\n",
        "            visualize_prediction_confidence_inline(\n",
        "                model_type=model_type,\n",
        "                target_name=target_name,\n",
        "                X_test=X_test,\n",
        "                y_test=y_test,\n",
        "                model=model\n",
        "            )\n",
        "\n",
        "    # Calculate average RMSE across folds\n",
        "    avg_rmse = np.mean(all_rmse)\n",
        "    print(f\"{model_type.upper()} - {target_name} - Average CV RMSE: {avg_rmse:.4f}\")\n",
        "\n",
        "    # Save the best model (with lowest RMSE)\n",
        "    best_model_idx = np.argmin(all_rmse)\n",
        "    best_model = all_models[best_model_idx]\n",
        "    best_model.save(f\"models/{model_type}{target_name.replace(' ', '')}_best.h5\")\n",
        "\n",
        "    return best_model, avg_rmse\n",
        "\n",
        "# Model comparison\n",
        "results = []\n",
        "\n",
        "# Train models for each target with cross-validation\n",
        "input_shape = (3, len(feature_columns))  # (sequence_length, num_features)\n",
        "\n",
        "# Store importance data for comparison across targets\n",
        "importance_data = {model_type: {} for model_type in ['mlp', 'lstm', 'transformer']}\n",
        "\n",
        "for target in target_columns:\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Training models for {target}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    for model_type in ['mlp', 'lstm', 'transformer']:\n",
        "        model, rmse = train_and_evaluate_cv(\n",
        "            model_type,\n",
        "            cv_splits,\n",
        "            input_shape,\n",
        "            target\n",
        "        )\n",
        "\n",
        "        results.append({\n",
        "            'Model': model_type,\n",
        "            'Target': target,\n",
        "            'RMSE': rmse\n",
        "        })\n",
        "\n",
        "        # Load feature importance if available\n",
        "        importance_file = f\"interpretability/{model_type}{target.replace(' ', '')}_importance.csv\"\n",
        "        if os.path.exists(importance_file):\n",
        "            imp_df = pd.read_csv(importance_file)\n",
        "            importance_data[model_type][target] = imp_df\n",
        "\n",
        "# Create comparison visualization with CV results\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(\"results/demo_model_strict_cv_comparison.csv\", index=False)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=results_df, x='Target', y='RMSE', hue='Model')\n",
        "plt.title(\"Model Performance Comparison with Strict Time-Based CV (RMSE)\")\n",
        "plt.ylabel(\"RMSE (lower is better)\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"plots/demo_model_strict_cv_comparison.png\")\n",
        "\n",
        "# Compare feature importance across targets for each model type\n",
        "print(\"\\nComparing feature importance across targets...\")\n",
        "for model_type in ['mlp', 'lstm', 'transformer']:\n",
        "    if importance_data[model_type]:\n",
        "        compare_importances(importance_data[model_type], model_type)\n",
        "\n",
        "print(\"\\nModel interpretability analysis completed. Results saved to the 'interpretability' directory.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c6Ioheo3X_yQ",
        "outputId": "476e253a-aa0d-4642-e3c4-a0bd8999fcd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 16264 sequences with shape (16264, 3, 28)\n",
            "Year distribution in sequences:\n",
            "Year\n",
            "2001    481\n",
            "2002    481\n",
            "2003    505\n",
            "2004    528\n",
            "2005    553\n",
            "2006    555\n",
            "2007    588\n",
            "2008    639\n",
            "2009    685\n",
            "2010    766\n",
            "2011    772\n",
            "2012    801\n",
            "2013    837\n",
            "2014    833\n",
            "2015    851\n",
            "2016    865\n",
            "2017    887\n",
            "2018    892\n",
            "2019    912\n",
            "2020    916\n",
            "2021    964\n",
            "2022    953\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Creating strict time-based cross-validation splits (no overlapping years)...\n",
            "Fold 1: Train years: 2001-2004, Test years: 2005-2008, Train size: 1995, Test size: 2335\n",
            "Fold 2: Train years: 2001-2008, Test years: 2009-2012, Train size: 4330, Test size: 3024\n",
            "Fold 3: Train years: 2001-2012, Test years: 2013-2016, Train size: 7354, Test size: 3386\n",
            "Fold 4: Train years: 2001-2016, Test years: 2017-2020, Train size: 10740, Test size: 3607\n",
            "Fold 5: Train years: 2001-2020, Test years: 2021-2022, Train size: 14347, Test size: 1917\n",
            "\n",
            "==================================================\n",
            "Training models for Target 1\n",
            "==================================================\n",
            "\n",
            "Training mlp for Target 1 - Fold 1/5\n",
            "Epoch 1/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 50ms/step - loss: 4963.4097 - mae: 55.2156 - val_loss: 4584.1309 - val_mae: 52.9145 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 4818.0205 - mae: 54.5188 - val_loss: 4425.6167 - val_mae: 52.2264 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4637.7651 - mae: 53.7186 - val_loss: 4205.8823 - val_mae: 51.3801 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4427.7017 - mae: 52.6655 - val_loss: 4044.0674 - val_mae: 50.8344 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4225.5347 - mae: 52.0160 - val_loss: 3977.1257 - val_mae: 50.8168 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4071.7380 - mae: 51.5117 - val_loss: 3945.9456 - val_mae: 50.7230 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4054.0491 - mae: 51.5935 - val_loss: 3959.8315 - val_mae: 50.7306 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3925.1296 - mae: 50.9322 - val_loss: 3961.8721 - val_mae: 50.8357 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4012.1951 - mae: 51.5843 - val_loss: 3959.4480 - val_mae: 50.8944 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3977.1355 - mae: 51.2408 - val_loss: 3948.8918 - val_mae: 50.9090 - learning_rate: 5.0000e-04\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "MLP - Target 1 - Fold 1 Test RMSE: 62.0615\n",
            "\n",
            "Training mlp for Target 1 - Fold 2/5\n",
            "Epoch 1/10\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 4251.7256 - mae: 50.4355 - val_loss: 4243.6611 - val_mae: 50.6727 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4049.9238 - mae: 49.8806 - val_loss: 4032.5806 - val_mae: 50.2118 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3890.8469 - mae: 49.5665 - val_loss: 3927.2148 - val_mae: 50.6468 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3763.0444 - mae: 49.3273 - val_loss: 3921.9092 - val_mae: 51.2575 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 3733.9607 - mae: 49.3001 - val_loss: 3914.9790 - val_mae: 51.1700 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 3708.5164 - mae: 49.2523 - val_loss: 3891.4500 - val_mae: 51.0046 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 3686.1313 - mae: 49.1708 - val_loss: 3887.7615 - val_mae: 51.0480 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 3651.8113 - mae: 48.8903 - val_loss: 3879.5259 - val_mae: 50.9155 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3614.2097 - mae: 48.6748 - val_loss: 3886.3469 - val_mae: 51.0216 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3565.6616 - mae: 48.1927 - val_loss: 3886.9790 - val_mae: 50.9020 - learning_rate: 0.0010\n",
            "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "MLP - Target 1 - Fold 2 Test RMSE: 55.8387\n",
            "\n",
            "Training mlp for Target 1 - Fold 3/5\n",
            "Epoch 1/10\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 3230.2349 - mae: 43.0151 - val_loss: 3480.5901 - val_mae: 45.0269 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3147.2434 - mae: 43.0313 - val_loss: 3421.7869 - val_mae: 45.0343 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3093.8638 - mae: 43.0280 - val_loss: 3396.6475 - val_mae: 45.0614 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3071.6160 - mae: 43.0659 - val_loss: 3392.5295 - val_mae: 45.0721 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3053.0725 - mae: 42.9526 - val_loss: 3389.3284 - val_mae: 45.0406 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 3049.4167 - mae: 42.8908 - val_loss: 3385.6799 - val_mae: 45.0569 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3006.2080 - mae: 42.5700 - val_loss: 3384.4421 - val_mae: 44.8722 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3027.3987 - mae: 42.6275 - val_loss: 3382.0725 - val_mae: 44.8629 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2998.7324 - mae: 42.4695 - val_loss: 3380.0225 - val_mae: 44.7957 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2948.1162 - mae: 42.1453 - val_loss: 3380.0815 - val_mae: 44.7278 - learning_rate: 0.0010\n",
            "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "MLP - Target 1 - Fold 3 Test RMSE: 51.6114\n",
            "\n",
            "Training mlp for Target 1 - Fold 4/5\n",
            "Epoch 1/10\n",
            "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 3069.5867 - mae: 41.7432 - val_loss: 3230.5947 - val_mae: 43.4316 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2952.0083 - mae: 41.9179 - val_loss: 3190.6082 - val_mae: 43.3177 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2919.9658 - mae: 41.8776 - val_loss: 3172.8572 - val_mae: 43.3237 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2903.0259 - mae: 41.7874 - val_loss: 3166.3291 - val_mae: 43.2541 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2879.2419 - mae: 41.6818 - val_loss: 3162.1912 - val_mae: 43.1592 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2855.9924 - mae: 41.4814 - val_loss: 3154.9805 - val_mae: 43.1110 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2849.9973 - mae: 41.4583 - val_loss: 3154.7334 - val_mae: 42.9803 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2835.3501 - mae: 41.3479 - val_loss: 3151.9526 - val_mae: 43.0034 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2808.9495 - mae: 41.1529 - val_loss: 3157.7969 - val_mae: 42.9510 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2803.8528 - mae: 41.1239 - val_loss: 3160.1184 - val_mae: 43.0625 - learning_rate: 0.0010\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "MLP - Target 1 - Fold 4 Test RMSE: 55.7946\n",
            "\n",
            "Training mlp for Target 1 - Fold 5/5\n",
            "Epoch 1/10\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 3110.4058 - mae: 42.2904 - val_loss: 3135.3696 - val_mae: 43.0712 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 2997.7263 - mae: 42.5096 - val_loss: 3109.2898 - val_mae: 43.0419 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2963.9968 - mae: 42.4090 - val_loss: 3099.9209 - val_mae: 43.0020 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2958.4048 - mae: 42.3753 - val_loss: 3092.0125 - val_mae: 42.9555 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2937.2175 - mae: 42.1789 - val_loss: 3090.2112 - val_mae: 42.9662 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2928.4275 - mae: 42.1804 - val_loss: 3088.2441 - val_mae: 42.9699 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2920.9556 - mae: 42.1346 - val_loss: 3084.9321 - val_mae: 42.9092 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2899.9436 - mae: 41.9836 - val_loss: 3085.1223 - val_mae: 42.8967 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 2873.6892 - mae: 41.8166 - val_loss: 3077.4224 - val_mae: 42.9096 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2875.3630 - mae: 41.7674 - val_loss: 3082.1707 - val_mae: 42.8762 - learning_rate: 0.0010\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "MLP - Target 1 - Fold 5 Test RMSE: 55.5325\n",
            "\n",
            "Analyzing feature importance for mlp on Target 1...\n",
            "\n",
            "Visualizing prediction confidence for mlp on Target 1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP - Target 1 - Average CV RMSE: 56.1677\n",
            "\n",
            "Training lstm for Target 1 - Fold 1/5\n",
            "Epoch 1/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 4952.2358 - mae: 55.2085 - val_loss: 4557.4141 - val_mae: 52.7991 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 4840.6636 - mae: 54.6821 - val_loss: 4472.7773 - val_mae: 52.4482 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 4745.8936 - mae: 54.2196 - val_loss: 4430.8271 - val_mae: 52.2761 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 4697.7524 - mae: 53.9551 - val_loss: 4391.0493 - val_mae: 52.1180 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 4670.0073 - mae: 53.8674 - val_loss: 4354.8940 - val_mae: 51.9759 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4638.5342 - mae: 53.6769 - val_loss: 4326.6821 - val_mae: 51.8774 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4605.5073 - mae: 53.5470 - val_loss: 4301.1509 - val_mae: 51.7860 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4562.5430 - mae: 53.2940 - val_loss: 4278.4238 - val_mae: 51.7160 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4546.6792 - mae: 53.2857 - val_loss: 4258.6445 - val_mae: 51.6563 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 4529.9297 - mae: 53.2735 - val_loss: 4238.8931 - val_mae: 51.5924 - learning_rate: 0.0010\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
            "LSTM - Target 1 - Fold 1 Test RMSE: 60.5401\n",
            "\n",
            "Training lstm for Target 1 - Fold 2/5\n",
            "Epoch 1/10\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 4293.5269 - mae: 50.5883 - val_loss: 4300.7690 - val_mae: 51.0677 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 4174.7495 - mae: 50.4988 - val_loss: 4210.7637 - val_mae: 51.0835 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 4112.0537 - mae: 50.4923 - val_loss: 4175.0410 - val_mae: 51.1463 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 4090.5054 - mae: 50.5729 - val_loss: 4151.7349 - val_mae: 51.2052 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 4069.5581 - mae: 50.6124 - val_loss: 4135.7886 - val_mae: 51.2606 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 4056.1221 - mae: 50.6218 - val_loss: 4124.1118 - val_mae: 51.3155 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 4047.7744 - mae: 50.7265 - val_loss: 4113.7114 - val_mae: 51.3839 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 4033.6265 - mae: 50.7814 - val_loss: 4105.4663 - val_mae: 51.4333 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 4027.8467 - mae: 50.8729 - val_loss: 4093.8623 - val_mae: 51.4460 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 4008.5632 - mae: 50.5410 - val_loss: 4049.4663 - val_mae: 51.0097 - learning_rate: 0.0010\n",
            "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
            "LSTM - Target 1 - Fold 2 Test RMSE: 46.9228\n",
            "\n",
            "Training lstm for Target 1 - Fold 3/5\n",
            "Epoch 1/10\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - loss: 3239.7913 - mae: 42.8862 - val_loss: 3506.6042 - val_mae: 45.1591 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 3183.5652 - mae: 43.1907 - val_loss: 3474.3650 - val_mae: 45.2922 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 3163.4226 - mae: 43.2571 - val_loss: 3463.0303 - val_mae: 45.2696 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 3149.9102 - mae: 43.1987 - val_loss: 3450.6362 - val_mae: 45.2742 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 3132.9023 - mae: 43.2042 - val_loss: 3444.0791 - val_mae: 45.3232 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 3120.9265 - mae: 43.1744 - val_loss: 3438.8542 - val_mae: 45.4150 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 3106.1277 - mae: 43.1403 - val_loss: 3429.2512 - val_mae: 45.3767 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 3098.9314 - mae: 43.1189 - val_loss: 3416.5315 - val_mae: 45.2908 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 3080.7585 - mae: 42.9756 - val_loss: 3410.0081 - val_mae: 45.3016 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 3067.0481 - mae: 42.9273 - val_loss: 3405.9912 - val_mae: 45.3046 - learning_rate: 0.0010\n",
            "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "LSTM - Target 1 - Fold 3 Test RMSE: 51.2849\n",
            "\n",
            "Training lstm for Target 1 - Fold 4/5\n",
            "Epoch 1/10\n",
            "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 3084.1729 - mae: 41.7592 - val_loss: 3291.5437 - val_mae: 43.6015 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 3009.1887 - mae: 42.0560 - val_loss: 3266.2800 - val_mae: 43.7292 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 2994.2996 - mae: 42.1985 - val_loss: 3253.7773 - val_mae: 43.6910 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 2977.7896 - mae: 42.0480 - val_loss: 3239.5718 - val_mae: 43.5583 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 2962.8259 - mae: 41.9946 - val_loss: 3227.3127 - val_mae: 43.3836 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 2945.5088 - mae: 41.8411 - val_loss: 3215.2007 - val_mae: 43.3531 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 2926.0229 - mae: 41.7551 - val_loss: 3202.1895 - val_mae: 43.2598 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 2908.7185 - mae: 41.6650 - val_loss: 3190.8237 - val_mae: 43.1941 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2903.8843 - mae: 41.6120 - val_loss: 3183.4956 - val_mae: 43.1882 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 2879.4575 - mae: 41.4628 - val_loss: 3174.2036 - val_mae: 43.2089 - learning_rate: 0.0010\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "LSTM - Target 1 - Fold 4 Test RMSE: 55.7213\n",
            "\n",
            "Training lstm for Target 1 - Fold 5/5\n",
            "Epoch 1/10\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 3125.7183 - mae: 42.2807 - val_loss: 3181.2690 - val_mae: 43.2633 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 3061.6931 - mae: 42.7254 - val_loss: 3162.4407 - val_mae: 43.3579 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 3047.1145 - mae: 42.7583 - val_loss: 3148.7302 - val_mae: 43.1297 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 3028.6387 - mae: 42.5399 - val_loss: 3133.2847 - val_mae: 43.0819 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 3004.5330 - mae: 42.4642 - val_loss: 3120.0898 - val_mae: 42.9576 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 2991.5667 - mae: 42.3777 - val_loss: 3108.8037 - val_mae: 42.9972 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 2969.6729 - mae: 42.2765 - val_loss: 3100.8118 - val_mae: 42.9555 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 2960.9529 - mae: 42.2811 - val_loss: 3090.8086 - val_mae: 42.9105 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 2941.8359 - mae: 42.1986 - val_loss: 3087.4087 - val_mae: 42.9480 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 2934.8621 - mae: 42.1453 - val_loss: 3086.5496 - val_mae: 42.8805 - learning_rate: 0.0010\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "LSTM - Target 1 - Fold 5 Test RMSE: 54.7985\n",
            "\n",
            "Analyzing feature importance for lstm on Target 1...\n",
            "\n",
            "Visualizing prediction confidence for lstm on Target 1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM - Target 1 - Average CV RMSE: 53.8535\n",
            "\n",
            "Training transformer for Target 1 - Fold 1/5\n",
            "Epoch 1/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 83ms/step - loss: 4867.9956 - mae: 54.8896 - val_loss: 4346.7314 - val_mae: 51.9864 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4557.9746 - mae: 53.3589 - val_loss: 4215.1367 - val_mae: 51.5234 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4417.4790 - mae: 52.7580 - val_loss: 4104.1240 - val_mae: 51.0796 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4302.8809 - mae: 52.3333 - val_loss: 4021.6440 - val_mae: 50.8119 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4246.1548 - mae: 52.2215 - val_loss: 3972.9314 - val_mae: 50.8416 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4190.0586 - mae: 52.1476 - val_loss: 3940.5884 - val_mae: 50.8955 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4137.1382 - mae: 51.9707 - val_loss: 3918.4556 - val_mae: 50.9210 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 4087.3425 - mae: 51.8653 - val_loss: 3907.7666 - val_mae: 50.9612 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4094.4197 - mae: 51.8835 - val_loss: 3907.6003 - val_mae: 51.0623 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 4063.0671 - mae: 51.7334 - val_loss: 3905.9487 - val_mae: 51.1047 - learning_rate: 0.0010\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
            "TRANSFORMER - Target 1 - Fold 1 Test RMSE: 62.3132\n",
            "\n",
            "Training transformer for Target 1 - Fold 2/5\n",
            "Epoch 1/10\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 50ms/step - loss: 4213.3853 - mae: 50.4817 - val_loss: 4136.5645 - val_mae: 51.1216 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 4029.3716 - mae: 50.5209 - val_loss: 4023.8865 - val_mae: 51.0374 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3926.0129 - mae: 50.2899 - val_loss: 3945.8911 - val_mae: 50.9295 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3867.9062 - mae: 50.0768 - val_loss: 3916.9951 - val_mae: 50.9552 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3835.1021 - mae: 49.9963 - val_loss: 3910.4224 - val_mae: 50.9292 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3802.5938 - mae: 49.7893 - val_loss: 3917.1069 - val_mae: 51.1631 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3781.5608 - mae: 49.6536 - val_loss: 3920.0178 - val_mae: 51.1037 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3766.9619 - mae: 49.5537 - val_loss: 3925.7371 - val_mae: 51.1482 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3734.9187 - mae: 49.3807 - val_loss: 3924.8223 - val_mae: 50.8721 - learning_rate: 5.0000e-04\n",
            "Epoch 10/10\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3723.4199 - mae: 49.1283 - val_loss: 3924.4844 - val_mae: 50.8675 - learning_rate: 5.0000e-04\n",
            "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
            "TRANSFORMER - Target 1 - Fold 2 Test RMSE: 51.2868\n",
            "\n",
            "Training transformer for Target 1 - Fold 3/5\n",
            "Epoch 1/10\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - loss: 3192.4458 - mae: 43.2259 - val_loss: 3446.5652 - val_mae: 45.4192 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3140.3240 - mae: 43.3898 - val_loss: 3429.4133 - val_mae: 45.3783 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3111.7832 - mae: 43.1907 - val_loss: 3423.2678 - val_mae: 45.4182 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3093.3066 - mae: 43.0943 - val_loss: 3414.6311 - val_mae: 45.3470 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3073.5383 - mae: 42.9655 - val_loss: 3408.1318 - val_mae: 45.4029 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3059.9670 - mae: 42.8647 - val_loss: 3401.2505 - val_mae: 45.3297 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3041.9575 - mae: 42.7570 - val_loss: 3393.5935 - val_mae: 45.2575 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 3026.7100 - mae: 42.6285 - val_loss: 3381.0544 - val_mae: 45.1604 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2998.9734 - mae: 42.4292 - val_loss: 3376.9194 - val_mae: 45.3263 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2978.0122 - mae: 42.3020 - val_loss: 3384.9307 - val_mae: 45.2244 - learning_rate: 0.0010\n",
            "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
            "TRANSFORMER - Target 1 - Fold 3 Test RMSE: 51.3410\n",
            "\n",
            "Training transformer for Target 1 - Fold 4/5\n",
            "Epoch 1/10\n",
            "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - loss: 3047.2432 - mae: 41.8741 - val_loss: 3231.9141 - val_mae: 43.6896 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2964.4119 - mae: 42.1185 - val_loss: 3208.0269 - val_mae: 43.4603 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2929.7373 - mae: 41.8806 - val_loss: 3197.5046 - val_mae: 43.3856 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2912.9458 - mae: 41.7737 - val_loss: 3189.1956 - val_mae: 43.2902 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2891.3223 - mae: 41.6052 - val_loss: 3181.7371 - val_mae: 43.1790 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2872.2253 - mae: 41.4806 - val_loss: 3175.4299 - val_mae: 43.1328 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2849.6895 - mae: 41.3012 - val_loss: 3168.9829 - val_mae: 43.1343 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2831.8892 - mae: 41.1651 - val_loss: 3173.7456 - val_mae: 43.1147 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2812.9026 - mae: 41.0408 - val_loss: 3179.5098 - val_mae: 43.2840 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2793.0732 - mae: 40.9362 - val_loss: 3182.7393 - val_mae: 43.1445 - learning_rate: 0.0010\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
            "TRANSFORMER - Target 1 - Fold 4 Test RMSE: 55.6759\n",
            "\n",
            "Training transformer for Target 1 - Fold 5/5\n",
            "Epoch 1/10\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 15ms/step - loss: 3102.0940 - mae: 42.4722 - val_loss: 3139.6843 - val_mae: 43.1756 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 3023.5486 - mae: 42.7646 - val_loss: 3117.0864 - val_mae: 43.0172 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2997.8040 - mae: 42.6187 - val_loss: 3106.0376 - val_mae: 42.9526 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2982.0918 - mae: 42.4850 - val_loss: 3097.7397 - val_mae: 42.9313 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2967.2747 - mae: 42.3559 - val_loss: 3093.6975 - val_mae: 42.9294 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 2953.2234 - mae: 42.2624 - val_loss: 3089.3027 - val_mae: 42.9659 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2935.1609 - mae: 42.1310 - val_loss: 3086.6680 - val_mae: 42.9517 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2920.8745 - mae: 42.0444 - val_loss: 3087.2795 - val_mae: 42.8876 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2903.7517 - mae: 41.9310 - val_loss: 3085.6641 - val_mae: 42.9279 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2885.7090 - mae: 41.8302 - val_loss: 3084.5098 - val_mae: 42.9101 - learning_rate: 0.0010\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
            "TRANSFORMER - Target 1 - Fold 5 Test RMSE: 54.9352\n",
            "\n",
            "Analyzing feature importance for transformer on Target 1...\n",
            "\n",
            "Visualizing prediction confidence for transformer on Target 1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRANSFORMER - Target 1 - Average CV RMSE: 55.1104\n",
            "\n",
            "Comparing feature importance across targets...\n",
            "\n",
            "Model interpretability analysis completed. Results saved to the 'interpretability' directory.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Target 2"
      ],
      "metadata": {
        "id": "Ki5KTXDXtVks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Create directories for saving models and results\n",
        "os.makedirs('models', exist_ok=True)\n",
        "os.makedirs('results', exist_ok=True)\n",
        "os.makedirs('plots', exist_ok=True)\n",
        "os.makedirs('interpretability', exist_ok=True)  # For interpretability results\n",
        "\n",
        "# ================= MODEL INTERPRETABILITY FUNCTIONS =================\n",
        "# Analyze feature importance using a permutation approach\n",
        "def analyze_feature_importance_inline(model_type, target_name, X_test, y_test, feature_names, model=None):\n",
        "    \"\"\"\n",
        "    Analyze and visualize feature importance directly without loading saved models\n",
        "    \"\"\"\n",
        "    print(f\"\\nAnalyzing feature importance for {model_type} on {target_name}...\")\n",
        "\n",
        "    if model is None:\n",
        "        print(f\"No model provided for {model_type} on {target_name}, skipping analysis\")\n",
        "        return None\n",
        "\n",
        "    # Create baseline prediction\n",
        "    baseline_pred = model.predict(X_test, verbose=0).flatten()\n",
        "    baseline_mse = mean_squared_error(y_test, baseline_pred)\n",
        "\n",
        "    # Calculate feature importance via permutation\n",
        "    feature_importances = []\n",
        "\n",
        "    # For sequence data, analyze each feature\n",
        "    seq_length, n_features = X_test.shape[1], X_test.shape[2]\n",
        "\n",
        "    for feat_idx in range(n_features):\n",
        "        # Copy the test data\n",
        "        X_permuted = X_test.copy()\n",
        "\n",
        "        # Permute this feature across all time steps\n",
        "        for t in range(seq_length):\n",
        "            X_permuted[:, t, feat_idx] = np.random.permutation(X_permuted[:, t, feat_idx])\n",
        "\n",
        "        # Make predictions with permuted feature\n",
        "        perm_pred = model.predict(X_permuted, verbose=0).flatten()\n",
        "        perm_mse = mean_squared_error(y_test, perm_pred)\n",
        "\n",
        "        # Importance is the increase in error when feature is permuted\n",
        "        importance = perm_mse - baseline_mse\n",
        "        feature_importances.append(importance)\n",
        "\n",
        "    # Create DataFrame with feature importances\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': feature_importances\n",
        "    })\n",
        "    importance_df = importance_df.sort_values('Importance', ascending=False)\n",
        "\n",
        "    # Save to CSV\n",
        "    importance_df.to_csv(f\"interpretability/{model_type}{target_name.replace(' ', '')}_importance.csv\", index=False)\n",
        "\n",
        "    # Visualize top features\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    top_features = importance_df.head(10)\n",
        "    sns.barplot(x='Importance', y='Feature', data=top_features, palette='viridis')\n",
        "    plt.title(f\"Top 10 Important Features for {target_name} ({model_type.upper()})\")\n",
        "    plt.xlabel('Permutation Importance (higher = more important)')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"interpretability/{model_type}{target_name.replace(' ', '')}_importance.png\")\n",
        "    plt.close()\n",
        "\n",
        "    return importance_df\n",
        "\n",
        "# Visualize prediction distribution and confidence\n",
        "def visualize_prediction_confidence_inline(model_type, target_name, X_test, y_test, model=None):\n",
        "    \"\"\"\n",
        "    Visualize model predictions, errors, and confidence directly without loading saved models\n",
        "    \"\"\"\n",
        "    print(f\"\\nVisualizing prediction confidence for {model_type} on {target_name}...\")\n",
        "\n",
        "    if model is None:\n",
        "        print(f\"No model provided for {model_type} on {target_name}, skipping visualization\")\n",
        "        return\n",
        "\n",
        "    # Generate predictions\n",
        "    y_pred = model.predict(X_test, verbose=0).flatten()\n",
        "\n",
        "    # Calculate absolute error for each prediction\n",
        "    abs_errors = np.abs(y_test - y_pred)\n",
        "\n",
        "    # Create dataframe for visualization\n",
        "    pred_df = pd.DataFrame({\n",
        "        'Actual': y_test,\n",
        "        'Predicted': y_pred,\n",
        "        'AbsError': abs_errors\n",
        "    })\n",
        "\n",
        "    # Create scatter plot with error as color\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    scatter = plt.scatter(pred_df['Actual'], pred_df['Predicted'],\n",
        "                         c=pred_df['AbsError'], cmap='viridis', alpha=0.7)\n",
        "\n",
        "    # Add perfect prediction line\n",
        "    min_val = min(pred_df['Actual'].min(), pred_df['Predicted'].min())\n",
        "    max_val = max(pred_df['Actual'].max(), pred_df['Predicted'].max())\n",
        "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', label='Perfect Prediction')\n",
        "\n",
        "    plt.colorbar(scatter, label='Absolute Error')\n",
        "    plt.xlabel('Actual Values')\n",
        "    plt.ylabel('Predicted Values')\n",
        "    plt.title(f'Prediction Confidence - {model_type.upper()} for {target_name}')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"interpretability/{model_type}{target_name.replace(' ', '')}_prediction_confidence.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # Create error distribution plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.histplot(pred_df['AbsError'], kde=True)\n",
        "    plt.xlabel('Absolute Error')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title(f'Error Distribution - {model_type.upper()} for {target_name}')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"interpretability/{model_type}{target_name.replace(' ', '')}_error_distribution.png\")\n",
        "    plt.close()\n",
        "\n",
        "# Create a function to compare importances across targets\n",
        "def compare_importances(importance_data, model_type):\n",
        "    \"\"\"Compare feature importance across different targets\"\"\"\n",
        "    # If we have data for all targets, create comparison\n",
        "    if len(importance_data) >= 2:  # Need at least 2 targets to compare\n",
        "        # Get top 5 features from each target\n",
        "        all_top_features = []\n",
        "        for target, imp_df in importance_data.items():\n",
        "            all_top_features.extend(imp_df.head(5)['Feature'].tolist())\n",
        "\n",
        "        # Create unique list\n",
        "        all_top_features = list(set(all_top_features))\n",
        "\n",
        "        # Create comparison dataframe\n",
        "        comparison_df = pd.DataFrame({'Feature': all_top_features})\n",
        "\n",
        "        # Add importance for each target\n",
        "        for target, imp_df in importance_data.items():\n",
        "            # Get importance values for these features\n",
        "            target_df = imp_df[imp_df['Feature'].isin(all_top_features)]\n",
        "            # Create a mapping from feature to importance\n",
        "            importance_map = dict(zip(target_df['Feature'], target_df['Importance']))\n",
        "\n",
        "            # Add to comparison df with the target name as column\n",
        "            comparison_df[target] = comparison_df['Feature'].map(importance_map).fillna(0)\n",
        "\n",
        "        # Save to CSV\n",
        "        comparison_df.to_csv(f\"interpretability/{model_type}_feature_importance_comparison.csv\", index=False)\n",
        "\n",
        "        # Create heatmap visualization\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        heatmap_df = comparison_df.set_index('Feature')\n",
        "        sns.heatmap(heatmap_df, annot=True, cmap='viridis', fmt='.3f')\n",
        "        plt.title(f'Feature Importance Comparison Across Targets - {model_type.upper()}')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"interpretability/{model_type}_feature_importance_heatmap.png\")\n",
        "        plt.close()\n",
        "\n",
        "# # Load the preprocessed dataset\n",
        "# data = pd.read_csv('processed_data.csv')\n",
        "# print(f\"Dataset loaded with shape: {data.shape}\")\n",
        "\n",
        "data = df2\n",
        "\n",
        "# # Define feature and target columns\n",
        "feature_columns = [f'Feature{i}' for i in range(1, 29)]\n",
        "target_columns = ['Target 2']\n",
        "\n",
        "# Sort data by Year and Company\n",
        "data = data.sort_values(['Company', 'Year'])\n",
        "\n",
        "# Prepare sequences for time-series modeling\n",
        "def prepare_sequences(df, feature_cols, target_cols, seq_length=3):\n",
        "    \"\"\"Prepare time series sequences for each company\"\"\"\n",
        "    companies = df['Company'].unique()\n",
        "    X_sequences = []\n",
        "    y_dict = {target: [] for target in target_cols}\n",
        "    companies_included = []\n",
        "    years_included = []\n",
        "\n",
        "    for company in companies:\n",
        "        company_data = df[df['Company'] == company]\n",
        "        if len(company_data) >= seq_length + 1:  # +1 because we need at least one target\n",
        "            company_X = company_data[feature_cols].values\n",
        "            company_years = company_data['Year'].values\n",
        "\n",
        "            for i in range(len(company_data) - seq_length):\n",
        "                X_sequences.append(company_X[i:i+seq_length])\n",
        "                companies_included.append(company)\n",
        "                years_included.append(company_years[i+seq_length-1])\n",
        "\n",
        "                for target in target_cols:\n",
        "                    y_dict[target].append(company_data[target].values[i+seq_length])\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    X_sequences = np.array(X_sequences)\n",
        "    for target in target_cols:\n",
        "        y_dict[target] = np.array(y_dict[target])\n",
        "\n",
        "    # Create tracking DataFrame for analysis\n",
        "    tracking_df = pd.DataFrame({\n",
        "        'Company': companies_included,\n",
        "        'Year': years_included\n",
        "    })\n",
        "\n",
        "    return X_sequences, y_dict, tracking_df\n",
        "\n",
        "# Create sequences\n",
        "X_sequences, y_dict, tracking_df = prepare_sequences(data, feature_columns, target_columns, seq_length=3)\n",
        "print(f\"Created {len(X_sequences)} sequences with shape {X_sequences.shape}\")\n",
        "\n",
        "# Print year distribution\n",
        "year_distribution = tracking_df['Year'].value_counts().sort_index()\n",
        "print(f\"Year distribution in sequences:\")\n",
        "print(year_distribution)\n",
        "\n",
        "# Implement strict non-overlapping time-based CV\n",
        "def strict_time_cv_split(X, y_dict, tracking_df, n_splits=5):\n",
        "    \"\"\"\n",
        "    Create strictly non-overlapping time-based CV splits\n",
        "    Ensures no year appears in both training and testing sets\n",
        "    \"\"\"\n",
        "    years = sorted(tracking_df['Year'].unique())\n",
        "    total_years = len(years)\n",
        "\n",
        "    # Calculate approximate number of years for each split\n",
        "    years_per_split = total_years // n_splits\n",
        "\n",
        "    cv_splits = []\n",
        "    for i in range(n_splits):\n",
        "        # Calculate year boundaries (no overlap)\n",
        "        if i < n_splits - 1:\n",
        "            train_end_year = years[min((i+1) * years_per_split - 1, total_years-1)]\n",
        "            test_start_year = years[min((i+1) * years_per_split, total_years-1)]\n",
        "            test_end_year = years[min((i+2) * years_per_split - 1, total_years-1)]\n",
        "        else:\n",
        "            # Last fold uses all remaining years for testing\n",
        "            train_end_year = years[min((i+1) * years_per_split - 1, total_years-1)]\n",
        "            test_start_year = years[min((i+1) * years_per_split, total_years-1)]\n",
        "            test_end_year = years[-1]\n",
        "\n",
        "        # Get indices for this split\n",
        "        train_indices = tracking_df[tracking_df['Year'] <= train_end_year].index\n",
        "        test_indices = tracking_df[(tracking_df['Year'] >= test_start_year) &\n",
        "                                   (tracking_df['Year'] <= test_end_year)].index\n",
        "\n",
        "        # Create the train/test split\n",
        "        X_train = X[train_indices]\n",
        "        X_test = X[test_indices]\n",
        "        y_train = {target: y_dict[target][train_indices] for target in y_dict}\n",
        "        y_test = {target: y_dict[target][test_indices] for target in y_dict}\n",
        "\n",
        "        cv_splits.append((X_train, y_train, X_test, y_test))\n",
        "\n",
        "        # Print information about this fold\n",
        "        train_years = tracking_df.iloc[train_indices]['Year'].unique()\n",
        "        test_years = tracking_df.iloc[test_indices]['Year'].unique()\n",
        "        print(f\"Fold {i+1}: Train years: {min(train_years)}-{max(train_years)}, \"\n",
        "              f\"Test years: {min(test_years)}-{max(test_years)}, \"\n",
        "              f\"Train size: {len(train_indices)}, Test size: {len(test_indices)}\")\n",
        "\n",
        "    return cv_splits\n",
        "\n",
        "# Create CV splits with strict time-based boundaries\n",
        "print(\"\\nCreating strict time-based cross-validation splits (no overlapping years)...\")\n",
        "cv_splits = strict_time_cv_split(X_sequences, y_dict, tracking_df, n_splits=5)\n",
        "\n",
        "# Model building functions\n",
        "def build_mlp_model(input_shape, target_name):\n",
        "    \"\"\"Build a Multi-Layer Perceptron model\"\"\"\n",
        "    input_layer = Input(shape=input_shape)\n",
        "    x = Flatten()(input_layer)\n",
        "\n",
        "    # MLP layers\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    output = Dense(1, activation='linear')(x)\n",
        "\n",
        "    # Use safe name without spaces\n",
        "    model = Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def build_lstm_model(input_shape, target_name):\n",
        "    \"\"\"Build an LSTM model\"\"\"\n",
        "    input_layer = Input(shape=input_shape)\n",
        "\n",
        "    # LSTM layers\n",
        "    x = LSTM(32, return_sequences=True)(input_layer)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    x = LSTM(16)(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    output = Dense(1, activation='linear')(x)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def build_transformer_model(input_shape, target_name):\n",
        "    \"\"\"Build a simple Transformer model\"\"\"\n",
        "    input_layer = Input(shape=input_shape)\n",
        "\n",
        "    # Self-attention\n",
        "    x = input_layer\n",
        "\n",
        "    # Transformer block\n",
        "    attn = MultiHeadAttention(num_heads=4, key_dim=8)(x, x)\n",
        "    x = LayerNormalization(epsilon=1e-6)(x + attn)\n",
        "\n",
        "    # Feed-forward network\n",
        "    ffn = Dense(32, activation='relu')(x)\n",
        "    ffn = Dense(input_shape[-1])(ffn)\n",
        "    x = LayerNormalization(epsilon=1e-6)(x + ffn)\n",
        "\n",
        "    # Flatten for final dense layers\n",
        "    x = Flatten()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    output = Dense(1, activation='linear')(x)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Train and evaluate with CV\n",
        "def train_and_evaluate_cv(model_type, cv_splits, input_shape, target_name):\n",
        "    \"\"\"Train and evaluate a model using cross-validation for a specific target\"\"\"\n",
        "    all_rmse = []\n",
        "    all_models = []\n",
        "\n",
        "    for fold_idx, (X_train, y_train_dict, X_test, y_test_dict) in enumerate(cv_splits):\n",
        "        # Get target-specific data\n",
        "        y_train = y_train_dict[target_name]\n",
        "        y_test = y_test_dict[target_name]\n",
        "\n",
        "        print(f\"\\nTraining {model_type} for {target_name} - Fold {fold_idx+1}/{len(cv_splits)}\")\n",
        "\n",
        "        # Build model based on type\n",
        "        if model_type == 'mlp':\n",
        "            model = build_mlp_model(input_shape, target_name)\n",
        "        elif model_type == 'lstm':\n",
        "            model = build_lstm_model(input_shape, target_name)\n",
        "        elif model_type == 'transformer':\n",
        "            model = build_transformer_model(input_shape, target_name)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
        "\n",
        "        callbacks = [\n",
        "            EarlyStopping(patience=5, restore_best_weights=True),\n",
        "            ReduceLROnPlateau(factor=0.5, patience=3, min_lr=1e-6)\n",
        "        ]\n",
        "\n",
        "        # Train model\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_split=0.2,\n",
        "            epochs=10,  # Limited epochs for demo\n",
        "            batch_size=32,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Evaluate on test data\n",
        "        y_pred = model.predict(X_test).flatten()\n",
        "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "        all_rmse.append(rmse)\n",
        "        all_models.append(model)\n",
        "\n",
        "        print(f\"{model_type.upper()} - {target_name} - Fold {fold_idx+1} Test RMSE: {rmse:.4f}\")\n",
        "\n",
        "        # Plot loss curves for this fold\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(history.history['loss'], label='Training Loss')\n",
        "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "        plt.title(f'{model_type.upper()} Loss for {target_name} - Fold {fold_idx+1}')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss (MSE)')\n",
        "        plt.legend()\n",
        "        plt.savefig(f\"plots/{model_type}{target_name.replace(' ', '')}_fold{fold_idx+1}_loss.png\")\n",
        "        plt.close()\n",
        "\n",
        "        # Save predictions for this fold\n",
        "        results_df = pd.DataFrame({\n",
        "            'Actual': y_test,\n",
        "            'Predicted': y_pred,\n",
        "            'Error': y_test - y_pred\n",
        "        })\n",
        "        results_df.to_csv(f\"results/{model_type}{target_name.replace(' ', '')}_fold{fold_idx+1}_predictions.csv\", index=False)\n",
        "\n",
        "        # For the last fold, perform model interpretability\n",
        "        if fold_idx == len(cv_splits) - 1:\n",
        "            # Analyze feature importance\n",
        "            analyze_feature_importance_inline(\n",
        "                model_type=model_type,\n",
        "                target_name=target_name,\n",
        "                X_test=X_test,\n",
        "                y_test=y_test,\n",
        "                feature_names=feature_columns,\n",
        "                model=model\n",
        "            )\n",
        "\n",
        "            # Visualize prediction confidence\n",
        "            visualize_prediction_confidence_inline(\n",
        "                model_type=model_type,\n",
        "                target_name=target_name,\n",
        "                X_test=X_test,\n",
        "                y_test=y_test,\n",
        "                model=model\n",
        "            )\n",
        "\n",
        "    # Calculate average RMSE across folds\n",
        "    avg_rmse = np.mean(all_rmse)\n",
        "    print(f\"{model_type.upper()} - {target_name} - Average CV RMSE: {avg_rmse:.4f}\")\n",
        "\n",
        "    # Save the best model (with lowest RMSE)\n",
        "    best_model_idx = np.argmin(all_rmse)\n",
        "    best_model = all_models[best_model_idx]\n",
        "    best_model.save(f\"models/{model_type}{target_name.replace(' ', '')}_best.h5\")\n",
        "\n",
        "    return best_model, avg_rmse\n",
        "\n",
        "# Model comparison\n",
        "results = []\n",
        "\n",
        "# Train models for each target with cross-validation\n",
        "input_shape = (3, len(feature_columns))  # (sequence_length, num_features)\n",
        "\n",
        "# Store importance data for comparison across targets\n",
        "importance_data = {model_type: {} for model_type in ['mlp', 'lstm', 'transformer']}\n",
        "\n",
        "for target in target_columns:\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Training models for {target}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    for model_type in ['mlp', 'lstm', 'transformer']:\n",
        "        model, rmse = train_and_evaluate_cv(\n",
        "            model_type,\n",
        "            cv_splits,\n",
        "            input_shape,\n",
        "            target\n",
        "        )\n",
        "\n",
        "        results.append({\n",
        "            'Model': model_type,\n",
        "            'Target': target,\n",
        "            'RMSE': rmse\n",
        "        })\n",
        "\n",
        "        # Load feature importance if available\n",
        "        importance_file = f\"interpretability/{model_type}{target.replace(' ', '')}_importance.csv\"\n",
        "        if os.path.exists(importance_file):\n",
        "            imp_df = pd.read_csv(importance_file)\n",
        "            importance_data[model_type][target] = imp_df\n",
        "\n",
        "# Create comparison visualization with CV results\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(\"results/demo_model_strict_cv_comparison.csv\", index=False)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=results_df, x='Target', y='RMSE', hue='Model')\n",
        "plt.title(\"Model Performance Comparison with Strict Time-Based CV (RMSE)\")\n",
        "plt.ylabel(\"RMSE (lower is better)\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"plots/demo_model_strict_cv_comparison.png\")\n",
        "\n",
        "# Compare feature importance across targets for each model type\n",
        "print(\"\\nComparing feature importance across targets...\")\n",
        "for model_type in ['mlp', 'lstm', 'transformer']:\n",
        "    if importance_data[model_type]:\n",
        "        compare_importances(importance_data[model_type], model_type)\n",
        "\n",
        "print(\"\\nModel interpretability analysis completed. Results saved to the 'interpretability' directory.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMwv68slsCJb",
        "outputId": "fc8d71f3-66fd-4a9d-d903-3ec1a0f3afc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 15062 sequences with shape (15062, 3, 28)\n",
            "Year distribution in sequences:\n",
            "Year\n",
            "2001    461\n",
            "2002    468\n",
            "2003    503\n",
            "2004    524\n",
            "2005    544\n",
            "2006    548\n",
            "2007    584\n",
            "2008    626\n",
            "2009    688\n",
            "2010    753\n",
            "2011    759\n",
            "2012    786\n",
            "2013    825\n",
            "2014    827\n",
            "2015    844\n",
            "2016    845\n",
            "2017    877\n",
            "2018    878\n",
            "2019    899\n",
            "2020    902\n",
            "2021    921\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Creating strict time-based cross-validation splits (no overlapping years)...\n",
            "Fold 1: Train years: 2001-2004, Test years: 2005-2008, Train size: 1956, Test size: 2302\n",
            "Fold 2: Train years: 2001-2008, Test years: 2009-2012, Train size: 4258, Test size: 2986\n",
            "Fold 3: Train years: 2001-2012, Test years: 2013-2016, Train size: 7244, Test size: 3341\n",
            "Fold 4: Train years: 2001-2016, Test years: 2017-2020, Train size: 10585, Test size: 3556\n",
            "Fold 5: Train years: 2001-2020, Test years: 2021-2021, Train size: 14141, Test size: 921\n",
            "\n",
            "==================================================\n",
            "Training models for Target 2\n",
            "==================================================\n",
            "\n",
            "Training mlp for Target 2 - Fold 1/5\n",
            "Epoch 1/10\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - loss: 14720.6221 - mae: 97.7505 - val_loss: 13890.0381 - val_mae: 93.5123 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 14440.1211 - mae: 97.0121 - val_loss: 13574.9326 - val_mae: 92.6966 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 13953.6504 - mae: 95.8598 - val_loss: 12842.2617 - val_mae: 90.7913 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 13146.6357 - mae: 93.9504 - val_loss: 11854.0967 - val_mae: 88.6657 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 12239.1338 - mae: 91.7668 - val_loss: 11274.8867 - val_mae: 87.7935 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 11616.6924 - mae: 90.5034 - val_loss: 11198.8633 - val_mae: 88.6137 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 11337.2188 - mae: 90.2554 - val_loss: 11154.1221 - val_mae: 88.5144 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 11366.8184 - mae: 90.2591 - val_loss: 11123.0693 - val_mae: 88.2962 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 10979.6680 - mae: 88.9706 - val_loss: 11119.2090 - val_mae: 88.8564 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 11073.3076 - mae: 89.2391 - val_loss: 11192.5049 - val_mae: 89.3401 - learning_rate: 0.0010\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "MLP - Target 2 - Fold 1 Test RMSE: 88.0469\n",
            "\n",
            "Training mlp for Target 2 - Fold 2/5\n",
            "Epoch 1/10\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 10900.6377 - mae: 81.2956 - val_loss: 10999.5742 - val_mae: 81.1020 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10380.6943 - mae: 80.2466 - val_loss: 10166.9785 - val_mae: 79.7346 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9540.2803 - mae: 78.6730 - val_loss: 9844.2578 - val_mae: 79.6674 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 9220.4854 - mae: 77.9888 - val_loss: 9776.6182 - val_mae: 79.9148 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9077.4727 - mae: 77.4624 - val_loss: 9784.2773 - val_mae: 80.1564 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8829.0752 - mae: 76.5433 - val_loss: 9729.2559 - val_mae: 79.6273 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 8787.0166 - mae: 76.3116 - val_loss: 9691.1504 - val_mae: 79.3692 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8891.9844 - mae: 76.7213 - val_loss: 9687.0938 - val_mae: 79.4814 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 8665.1455 - mae: 75.4269 - val_loss: 9691.5859 - val_mae: 79.3678 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 8797.4766 - mae: 76.0800 - val_loss: 9673.0449 - val_mae: 79.7444 - learning_rate: 0.0010\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "MLP - Target 2 - Fold 2 Test RMSE: 89.8452\n",
            "\n",
            "Training mlp for Target 2 - Fold 3/5\n",
            "Epoch 1/10\n",
            "\u001b[1m182/182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 8923.8633 - mae: 71.5999 - val_loss: 9416.7412 - val_mae: 74.4959 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m182/182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 8423.9795 - mae: 71.5659 - val_loss: 8983.9717 - val_mae: 74.2401 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m182/182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 8130.6138 - mae: 71.7810 - val_loss: 8894.4082 - val_mae: 74.0499 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m182/182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 7999.9102 - mae: 71.5338 - val_loss: 8843.6348 - val_mae: 73.8791 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m182/182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 7949.4731 - mae: 71.1624 - val_loss: 8802.7969 - val_mae: 73.6898 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m182/182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 7864.7251 - mae: 70.7773 - val_loss: 8785.4355 - val_mae: 73.6476 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m182/182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 7800.9644 - mae: 70.4391 - val_loss: 8746.9092 - val_mae: 73.6405 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m182/182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 7752.7988 - mae: 70.1424 - val_loss: 8745.9502 - val_mae: 73.4528 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m182/182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 7632.8022 - mae: 69.5121 - val_loss: 8733.6660 - val_mae: 73.3787 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m182/182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 7634.2144 - mae: 69.4112 - val_loss: 8715.5303 - val_mae: 73.3319 - learning_rate: 0.0010\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "MLP - Target 2 - Fold 3 Test RMSE: 78.7707\n",
            "\n",
            "Training mlp for Target 2 - Fold 4/5\n",
            "Epoch 1/10\n",
            "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 7945.0996 - mae: 67.3649 - val_loss: 8394.7900 - val_mae: 70.7528 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 7446.8613 - mae: 67.4462 - val_loss: 8131.1997 - val_mae: 70.3803 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 7288.9619 - mae: 67.4154 - val_loss: 8062.5396 - val_mae: 70.1631 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 7182.5908 - mae: 66.9393 - val_loss: 8022.9551 - val_mae: 70.1144 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 7158.9282 - mae: 66.6743 - val_loss: 8009.9668 - val_mae: 69.7430 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 7062.4561 - mae: 66.0386 - val_loss: 7995.4854 - val_mae: 69.7277 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 7001.5454 - mae: 65.9026 - val_loss: 7974.1143 - val_mae: 69.6971 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 6987.2900 - mae: 65.6931 - val_loss: 7960.5527 - val_mae: 69.5135 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 6920.5630 - mae: 65.3772 - val_loss: 7951.9697 - val_mae: 69.6616 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 6767.0225 - mae: 64.7801 - val_loss: 7977.6592 - val_mae: 69.5366 - learning_rate: 0.0010\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "MLP - Target 2 - Fold 4 Test RMSE: 85.4001\n",
            "\n",
            "Training mlp for Target 2 - Fold 5/5\n",
            "Epoch 1/10\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - loss: 7629.5000 - mae: 65.7843 - val_loss: 7937.9937 - val_mae: 68.4105 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 7265.3882 - mae: 66.3310 - val_loss: 7765.4868 - val_mae: 68.2888 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 7218.0820 - mae: 66.5696 - val_loss: 7706.2686 - val_mae: 68.0480 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 7157.5967 - mae: 66.1693 - val_loss: 7687.7271 - val_mae: 67.8466 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 7123.1943 - mae: 65.9593 - val_loss: 7671.9980 - val_mae: 67.8502 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 7084.1621 - mae: 65.7662 - val_loss: 7647.0625 - val_mae: 67.7667 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 7036.4678 - mae: 65.4673 - val_loss: 7660.6187 - val_mae: 67.9903 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 7028.0542 - mae: 65.4860 - val_loss: 7634.0068 - val_mae: 67.8126 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 6981.1709 - mae: 65.2804 - val_loss: 7658.4497 - val_mae: 67.9985 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 6949.4126 - mae: 64.9915 - val_loss: 7672.4951 - val_mae: 67.7405 - learning_rate: 0.0010\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step\n",
            "MLP - Target 2 - Fold 5 Test RMSE: 90.7813\n",
            "\n",
            "Analyzing feature importance for mlp on Target 2...\n",
            "\n",
            "Visualizing prediction confidence for mlp on Target 2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP - Target 2 - Average CV RMSE: 86.5689\n",
            "\n",
            "Training lstm for Target 2 - Fold 1/5\n",
            "Epoch 1/10\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 14729.7832 - mae: 97.8253 - val_loss: 13801.5664 - val_mae: 93.4084 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 14484.3623 - mae: 97.3824 - val_loss: 13576.2119 - val_mae: 92.9901 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 14311.1836 - mae: 97.1636 - val_loss: 13484.1631 - val_mae: 92.7859 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 14224.3965 - mae: 97.0278 - val_loss: 13398.4785 - val_mae: 92.5727 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 14130.5078 - mae: 96.8029 - val_loss: 13312.9795 - val_mae: 92.3737 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 14040.7158 - mae: 96.6721 - val_loss: 13240.3721 - val_mae: 92.2019 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 13995.4580 - mae: 96.5996 - val_loss: 13180.1338 - val_mae: 92.0720 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 13931.2334 - mae: 96.5347 - val_loss: 13120.2656 - val_mae: 91.9443 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 13858.9180 - mae: 96.3359 - val_loss: 13067.5430 - val_mae: 91.8440 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 13797.6758 - mae: 96.2175 - val_loss: 13023.0762 - val_mae: 91.7740 - learning_rate: 0.0010\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
            "LSTM - Target 2 - Fold 1 Test RMSE: 85.7180\n",
            "\n",
            "Training lstm for Target 2 - Fold 2/5\n",
            "Epoch 1/10\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 10985.6924 - mae: 81.4352 - val_loss: 11125.4541 - val_mae: 81.6291 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 10743.4014 - mae: 81.3845 - val_loss: 10958.1436 - val_mae: 81.4710 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 10619.5439 - mae: 81.3235 - val_loss: 10872.1045 - val_mae: 81.4166 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 10548.9141 - mae: 81.3676 - val_loss: 10799.1104 - val_mae: 81.1983 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 10460.7422 - mae: 81.1009 - val_loss: 10726.5078 - val_mae: 81.1889 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 10399.4092 - mae: 80.9370 - val_loss: 10656.3789 - val_mae: 80.8607 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 10325.1328 - mae: 80.6500 - val_loss: 10583.9287 - val_mae: 80.6023 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 10257.1826 - mae: 80.3945 - val_loss: 10515.3721 - val_mae: 80.2536 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 10184.3965 - mae: 80.1711 - val_loss: 10451.1172 - val_mae: 79.9870 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 10116.1904 - mae: 79.9479 - val_loss: 10384.4756 - val_mae: 79.8042 - learning_rate: 0.0010\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
            "LSTM - Target 2 - Fold 2 Test RMSE: 78.7299\n",
            "\n",
            "Training lstm for Target 2 - Fold 3/5\n",
            "Epoch 1/10\n",
            "\u001b[1m182/182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 8954.4316 - mae: 71.7473 - val_loss: 9579.5264 - val_mae: 75.0529 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m182/182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 8764.6357 - mae: 72.1195 - val_loss: 9468.7158 - val_mae: 74.9055 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m182/182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 8677.9629 - mae: 72.0246 - val_loss: 9400.7373 - val_mae: 74.9023 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m182/182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 8629.9297 - mae: 72.0514 - val_loss: 9349.4873 - val_mae: 74.9819 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m182/182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 8576.3672 - mae: 72.0375 - val_loss: 9298.7451 - val_mae: 74.9601 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m182/182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 8522.0566 - mae: 71.9352 - val_loss: 9255.3555 - val_mae: 74.8407 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m182/182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 8443.2920 - mae: 71.6987 - val_loss: 9215.7090 - val_mae: 75.0443 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m182/182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 8416.0947 - mae: 71.7447 - val_loss: 9164.7080 - val_mae: 74.6429 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m182/182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 8348.2383 - mae: 71.4243 - val_loss: 9139.4756 - val_mae: 74.7747 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m182/182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 8298.5420 - mae: 71.1977 - val_loss: 9087.5068 - val_mae: 74.2775 - learning_rate: 0.0010\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "LSTM - Target 2 - Fold 3 Test RMSE: 78.3177\n",
            "\n",
            "Training lstm for Target 2 - Fold 4/5\n",
            "Epoch 1/10\n",
            "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 8006.8994 - mae: 67.4172 - val_loss: 8729.9561 - val_mae: 71.1305 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 7811.7236 - mae: 67.7323 - val_loss: 8611.9883 - val_mae: 71.0519 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 7720.9355 - mae: 67.6666 - val_loss: 8540.8340 - val_mae: 70.8222 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 7645.2622 - mae: 67.4470 - val_loss: 8460.8926 - val_mae: 70.6864 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 7569.1499 - mae: 67.3459 - val_loss: 8402.9697 - val_mae: 70.5955 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 7489.3989 - mae: 67.0394 - val_loss: 8350.0107 - val_mae: 70.7263 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 7422.2983 - mae: 66.9203 - val_loss: 8315.7109 - val_mae: 70.8439 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 7340.3052 - mae: 66.5607 - val_loss: 8291.4453 - val_mae: 70.8316 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 7267.2173 - mae: 66.1921 - val_loss: 8267.4639 - val_mae: 70.8824 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 7234.4209 - mae: 66.2052 - val_loss: 8247.0908 - val_mae: 71.1203 - learning_rate: 0.0010\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "LSTM - Target 2 - Fold 4 Test RMSE: 85.2765\n",
            "\n",
            "Training lstm for Target 2 - Fold 5/5\n",
            "Epoch 1/10\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 7701.8071 - mae: 65.7145 - val_loss: 8168.5508 - val_mae: 69.1226 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 7523.6035 - mae: 66.3385 - val_loss: 8083.5127 - val_mae: 69.2890 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 7466.2056 - mae: 66.4061 - val_loss: 8023.5908 - val_mae: 69.3984 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 7405.5654 - mae: 66.3213 - val_loss: 7965.7329 - val_mae: 69.2929 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 7349.0107 - mae: 66.0997 - val_loss: 7922.1133 - val_mae: 69.1175 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 7298.1187 - mae: 66.0289 - val_loss: 7889.7495 - val_mae: 68.9160 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 7252.1558 - mae: 65.8693 - val_loss: 7867.1416 - val_mae: 68.7992 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 7203.4033 - mae: 65.6026 - val_loss: 7846.3706 - val_mae: 68.8301 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 7156.7168 - mae: 65.4290 - val_loss: 7831.9243 - val_mae: 68.5258 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 7133.2275 - mae: 65.2232 - val_loss: 7830.7998 - val_mae: 68.3736 - learning_rate: 0.0010\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "LSTM - Target 2 - Fold 5 Test RMSE: 90.2639\n",
            "\n",
            "Analyzing feature importance for lstm on Target 2...\n",
            "\n",
            "Visualizing prediction confidence for lstm on Target 2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM - Target 2 - Average CV RMSE: 83.6612\n",
            "\n",
            "Training transformer for Target 2 - Fold 1/5\n",
            "Epoch 1/10\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 80ms/step - loss: 14546.2627 - mae: 97.4442 - val_loss: 13391.9121 - val_mae: 92.5831 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14035.7783 - mae: 96.6508 - val_loss: 13021.4365 - val_mae: 91.8348 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 13656.1475 - mae: 96.0742 - val_loss: 12648.4658 - val_mae: 91.2130 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 13289.7139 - mae: 95.4423 - val_loss: 12285.7617 - val_mae: 90.5681 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12911.6826 - mae: 94.7098 - val_loss: 11991.9697 - val_mae: 90.2055 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 12625.4688 - mae: 94.2299 - val_loss: 11773.5615 - val_mae: 90.0355 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 12315.0273 - mae: 93.5798 - val_loss: 11605.2119 - val_mae: 90.0241 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 12112.3564 - mae: 93.2899 - val_loss: 11484.3984 - val_mae: 90.1382 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 11947.4238 - mae: 93.0568 - val_loss: 11402.2402 - val_mae: 90.2247 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 11858.3594 - mae: 92.9446 - val_loss: 11343.0137 - val_mae: 90.2533 - learning_rate: 0.0010\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
            "TRANSFORMER - Target 2 - Fold 1 Test RMSE: 89.9966\n",
            "\n",
            "Training transformer for Target 2 - Fold 2/5\n",
            "Epoch 1/10\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 43ms/step - loss: 10830.2578 - mae: 81.2275 - val_loss: 10677.6807 - val_mae: 81.0210 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 10239.3252 - mae: 80.6487 - val_loss: 10235.0137 - val_mae: 79.9066 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 9809.1582 - mae: 79.4506 - val_loss: 9971.5078 - val_mae: 78.8168 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9478.8037 - mae: 78.4954 - val_loss: 9863.9258 - val_mae: 78.5421 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 9323.8477 - mae: 77.9455 - val_loss: 9800.8145 - val_mae: 78.5502 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9146.2080 - mae: 77.3770 - val_loss: 9749.6270 - val_mae: 78.5566 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9023.8477 - mae: 76.8740 - val_loss: 9725.2305 - val_mae: 78.5167 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 8956.0518 - mae: 76.7160 - val_loss: 9708.7305 - val_mae: 78.4885 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 8863.2393 - mae: 76.3200 - val_loss: 9706.1729 - val_mae: 78.4822 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 8822.4326 - mae: 76.0345 - val_loss: 9728.5801 - val_mae: 78.5188 - learning_rate: 0.0010\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
            "TRANSFORMER - Target 2 - Fold 2 Test RMSE: 87.9891\n",
            "\n",
            "Training transformer for Target 2 - Fold 3/5\n",
            "Epoch 1/10\n",
            "\u001b[1m182/182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - loss: 8871.0098 - mae: 71.8920 - val_loss: 9324.7822 - val_mae: 74.6990 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m182/182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 8524.4033 - mae: 72.1074 - val_loss: 9095.3379 - val_mae: 74.3200 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m182/182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 8310.2979 - mae: 71.7283 - val_loss: 9012.7783 - val_mae: 74.1124 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m182/182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 8174.8267 - mae: 71.4751 - val_loss: 8992.6562 - val_mae: 74.0321 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m182/182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 8079.3208 - mae: 71.0569 - val_loss: 8952.2236 - val_mae: 74.0755 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m182/182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 7980.9546 - mae: 70.7159 - val_loss: 8932.6055 - val_mae: 74.0659 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m182/182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 7930.9111 - mae: 70.4885 - val_loss: 8920.1484 - val_mae: 74.0343 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m182/182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 7854.2676 - mae: 70.2012 - val_loss: 8909.5176 - val_mae: 73.9644 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m182/182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 7792.1948 - mae: 69.7840 - val_loss: 8911.4570 - val_mae: 74.0417 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m182/182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 7733.1567 - mae: 69.5422 - val_loss: 8920.5869 - val_mae: 73.9983 - learning_rate: 0.0010\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
            "TRANSFORMER - Target 2 - Fold 3 Test RMSE: 78.0383\n",
            "\n",
            "Training transformer for Target 2 - Fold 4/5\n",
            "Epoch 1/10\n",
            "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 7855.0029 - mae: 67.5228 - val_loss: 8377.0908 - val_mae: 70.8912 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 7495.2222 - mae: 67.6492 - val_loss: 8230.1875 - val_mae: 70.7126 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 7340.2344 - mae: 67.3639 - val_loss: 8177.2773 - val_mae: 70.8499 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 7269.8984 - mae: 67.1146 - val_loss: 8149.5259 - val_mae: 70.8760 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 7194.1929 - mae: 66.7957 - val_loss: 8111.9375 - val_mae: 70.9426 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 7118.6919 - mae: 66.5141 - val_loss: 8079.4897 - val_mae: 70.9696 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 7064.6362 - mae: 66.0988 - val_loss: 8061.3301 - val_mae: 70.9394 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 7002.8955 - mae: 65.8135 - val_loss: 8039.8687 - val_mae: 70.8472 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 6937.8608 - mae: 65.4089 - val_loss: 8020.6479 - val_mae: 70.8423 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 6893.0518 - mae: 65.2085 - val_loss: 8018.0010 - val_mae: 70.6154 - learning_rate: 0.0010\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step\n",
            "TRANSFORMER - Target 2 - Fold 4 Test RMSE: 85.9499\n",
            "\n",
            "Training transformer for Target 2 - Fold 5/5\n",
            "Epoch 1/10\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - loss: 7607.8301 - mae: 65.9866 - val_loss: 7945.4575 - val_mae: 69.4737 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 7347.8296 - mae: 66.6908 - val_loss: 7854.4961 - val_mae: 68.8871 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 7238.5698 - mae: 66.2405 - val_loss: 7829.7485 - val_mae: 68.4702 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 7157.8789 - mae: 65.7702 - val_loss: 7784.1299 - val_mae: 68.3406 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 7098.4243 - mae: 65.4318 - val_loss: 7760.1772 - val_mae: 68.2356 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 7035.9023 - mae: 65.1840 - val_loss: 7751.4351 - val_mae: 68.1478 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 6985.2129 - mae: 64.9265 - val_loss: 7735.6606 - val_mae: 68.1686 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 6960.7100 - mae: 64.7962 - val_loss: 7735.7690 - val_mae: 68.0986 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 6902.0073 - mae: 64.5839 - val_loss: 7749.1982 - val_mae: 68.1547 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 6849.5498 - mae: 64.2832 - val_loss: 7752.2256 - val_mae: 68.1676 - learning_rate: 0.0010\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step\n",
            "TRANSFORMER - Target 2 - Fold 5 Test RMSE: 90.3122\n",
            "\n",
            "Analyzing feature importance for transformer on Target 2...\n",
            "\n",
            "Visualizing prediction confidence for transformer on Target 2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRANSFORMER - Target 2 - Average CV RMSE: 86.4572\n",
            "\n",
            "Comparing feature importance across targets...\n",
            "\n",
            "Model interpretability analysis completed. Results saved to the 'interpretability' directory.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Target 3"
      ],
      "metadata": {
        "id": "_QWBvjQltnWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Create directories for saving models and results\n",
        "os.makedirs('models', exist_ok=True)\n",
        "os.makedirs('results', exist_ok=True)\n",
        "os.makedirs('plots', exist_ok=True)\n",
        "os.makedirs('interpretability', exist_ok=True)  # For interpretability results\n",
        "\n",
        "# ================= MODEL INTERPRETABILITY FUNCTIONS =================\n",
        "# Analyze feature importance using a permutation approach\n",
        "def analyze_feature_importance_inline(model_type, target_name, X_test, y_test, feature_names, model=None):\n",
        "    \"\"\"\n",
        "    Analyze and visualize feature importance directly without loading saved models\n",
        "    \"\"\"\n",
        "    print(f\"\\nAnalyzing feature importance for {model_type} on {target_name}...\")\n",
        "\n",
        "    if model is None:\n",
        "        print(f\"No model provided for {model_type} on {target_name}, skipping analysis\")\n",
        "        return None\n",
        "\n",
        "    # Create baseline prediction\n",
        "    baseline_pred = model.predict(X_test, verbose=0).flatten()\n",
        "    baseline_mse = mean_squared_error(y_test, baseline_pred)\n",
        "\n",
        "    # Calculate feature importance via permutation\n",
        "    feature_importances = []\n",
        "\n",
        "    # For sequence data, analyze each feature\n",
        "    seq_length, n_features = X_test.shape[1], X_test.shape[2]\n",
        "\n",
        "    for feat_idx in range(n_features):\n",
        "        # Copy the test data\n",
        "        X_permuted = X_test.copy()\n",
        "\n",
        "        # Permute this feature across all time steps\n",
        "        for t in range(seq_length):\n",
        "            X_permuted[:, t, feat_idx] = np.random.permutation(X_permuted[:, t, feat_idx])\n",
        "\n",
        "        # Make predictions with permuted feature\n",
        "        perm_pred = model.predict(X_permuted, verbose=0).flatten()\n",
        "        perm_mse = mean_squared_error(y_test, perm_pred)\n",
        "\n",
        "        # Importance is the increase in error when feature is permuted\n",
        "        importance = perm_mse - baseline_mse\n",
        "        feature_importances.append(importance)\n",
        "\n",
        "    # Create DataFrame with feature importances\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': feature_importances\n",
        "    })\n",
        "    importance_df = importance_df.sort_values('Importance', ascending=False)\n",
        "\n",
        "    # Save to CSV\n",
        "    importance_df.to_csv(f\"interpretability/{model_type}{target_name.replace(' ', '')}_importance.csv\", index=False)\n",
        "\n",
        "    # Visualize top features\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    top_features = importance_df.head(10)\n",
        "    sns.barplot(x='Importance', y='Feature', data=top_features, palette='viridis')\n",
        "    plt.title(f\"Top 10 Important Features for {target_name} ({model_type.upper()})\")\n",
        "    plt.xlabel('Permutation Importance (higher = more important)')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"interpretability/{model_type}{target_name.replace(' ', '')}_importance.png\")\n",
        "    plt.close()\n",
        "\n",
        "    return importance_df\n",
        "\n",
        "# Visualize prediction distribution and confidence\n",
        "def visualize_prediction_confidence_inline(model_type, target_name, X_test, y_test, model=None):\n",
        "    \"\"\"\n",
        "    Visualize model predictions, errors, and confidence directly without loading saved models\n",
        "    \"\"\"\n",
        "    print(f\"\\nVisualizing prediction confidence for {model_type} on {target_name}...\")\n",
        "\n",
        "    if model is None:\n",
        "        print(f\"No model provided for {model_type} on {target_name}, skipping visualization\")\n",
        "        return\n",
        "\n",
        "    # Generate predictions\n",
        "    y_pred = model.predict(X_test, verbose=0).flatten()\n",
        "\n",
        "    # Calculate absolute error for each prediction\n",
        "    abs_errors = np.abs(y_test - y_pred)\n",
        "\n",
        "    # Create dataframe for visualization\n",
        "    pred_df = pd.DataFrame({\n",
        "        'Actual': y_test,\n",
        "        'Predicted': y_pred,\n",
        "        'AbsError': abs_errors\n",
        "    })\n",
        "\n",
        "    # Create scatter plot with error as color\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    scatter = plt.scatter(pred_df['Actual'], pred_df['Predicted'],\n",
        "                         c=pred_df['AbsError'], cmap='viridis', alpha=0.7)\n",
        "\n",
        "    # Add perfect prediction line\n",
        "    min_val = min(pred_df['Actual'].min(), pred_df['Predicted'].min())\n",
        "    max_val = max(pred_df['Actual'].max(), pred_df['Predicted'].max())\n",
        "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', label='Perfect Prediction')\n",
        "\n",
        "    plt.colorbar(scatter, label='Absolute Error')\n",
        "    plt.xlabel('Actual Values')\n",
        "    plt.ylabel('Predicted Values')\n",
        "    plt.title(f'Prediction Confidence - {model_type.upper()} for {target_name}')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"interpretability/{model_type}{target_name.replace(' ', '')}_prediction_confidence.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # Create error distribution plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.histplot(pred_df['AbsError'], kde=True)\n",
        "    plt.xlabel('Absolute Error')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title(f'Error Distribution - {model_type.upper()} for {target_name}')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"interpretability/{model_type}{target_name.replace(' ', '')}_error_distribution.png\")\n",
        "    plt.close()\n",
        "\n",
        "# Create a function to compare importances across targets\n",
        "def compare_importances(importance_data, model_type):\n",
        "    \"\"\"Compare feature importance across different targets\"\"\"\n",
        "    # If we have data for all targets, create comparison\n",
        "    if len(importance_data) >= 2:  # Need at least 2 targets to compare\n",
        "        # Get top 5 features from each target\n",
        "        all_top_features = []\n",
        "        for target, imp_df in importance_data.items():\n",
        "            all_top_features.extend(imp_df.head(5)['Feature'].tolist())\n",
        "\n",
        "        # Create unique list\n",
        "        all_top_features = list(set(all_top_features))\n",
        "\n",
        "        # Create comparison dataframe\n",
        "        comparison_df = pd.DataFrame({'Feature': all_top_features})\n",
        "\n",
        "        # Add importance for each target\n",
        "        for target, imp_df in importance_data.items():\n",
        "            # Get importance values for these features\n",
        "            target_df = imp_df[imp_df['Feature'].isin(all_top_features)]\n",
        "            # Create a mapping from feature to importance\n",
        "            importance_map = dict(zip(target_df['Feature'], target_df['Importance']))\n",
        "\n",
        "            # Add to comparison df with the target name as column\n",
        "            comparison_df[target] = comparison_df['Feature'].map(importance_map).fillna(0)\n",
        "\n",
        "        # Save to CSV\n",
        "        comparison_df.to_csv(f\"interpretability/{model_type}_feature_importance_comparison.csv\", index=False)\n",
        "\n",
        "        # Create heatmap visualization\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        heatmap_df = comparison_df.set_index('Feature')\n",
        "        sns.heatmap(heatmap_df, annot=True, cmap='viridis', fmt='.3f')\n",
        "        plt.title(f'Feature Importance Comparison Across Targets - {model_type.upper()}')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"interpretability/{model_type}_feature_importance_heatmap.png\")\n",
        "        plt.close()\n",
        "\n",
        "# # Load the preprocessed dataset\n",
        "# data = pd.read_csv('processed_data.csv')\n",
        "# print(f\"Dataset loaded with shape: {data.shape}\")\n",
        "\n",
        "data = df3\n",
        "\n",
        "# # Define feature and target columns\n",
        "feature_columns = [f'Feature{i}' for i in range(1, 29)]\n",
        "target_columns = ['Target 3']\n",
        "\n",
        "# Sort data by Year and Company\n",
        "data = data.sort_values(['Company', 'Year'])\n",
        "\n",
        "# Prepare sequences for time-series modeling\n",
        "def prepare_sequences(df, feature_cols, target_cols, seq_length=3):\n",
        "    \"\"\"Prepare time series sequences for each company\"\"\"\n",
        "    companies = df['Company'].unique()\n",
        "    X_sequences = []\n",
        "    y_dict = {target: [] for target in target_cols}\n",
        "    companies_included = []\n",
        "    years_included = []\n",
        "\n",
        "    for company in companies:\n",
        "        company_data = df[df['Company'] == company]\n",
        "        if len(company_data) >= seq_length + 1:  # +1 because we need at least one target\n",
        "            company_X = company_data[feature_cols].values\n",
        "            company_years = company_data['Year'].values\n",
        "\n",
        "            for i in range(len(company_data) - seq_length):\n",
        "                X_sequences.append(company_X[i:i+seq_length])\n",
        "                companies_included.append(company)\n",
        "                years_included.append(company_years[i+seq_length-1])\n",
        "\n",
        "                for target in target_cols:\n",
        "                    y_dict[target].append(company_data[target].values[i+seq_length])\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    X_sequences = np.array(X_sequences)\n",
        "    for target in target_cols:\n",
        "        y_dict[target] = np.array(y_dict[target])\n",
        "\n",
        "    # Create tracking DataFrame for analysis\n",
        "    tracking_df = pd.DataFrame({\n",
        "        'Company': companies_included,\n",
        "        'Year': years_included\n",
        "    })\n",
        "\n",
        "    return X_sequences, y_dict, tracking_df\n",
        "\n",
        "# Create sequences\n",
        "X_sequences, y_dict, tracking_df = prepare_sequences(data, feature_columns, target_columns, seq_length=3)\n",
        "print(f\"Created {len(X_sequences)} sequences with shape {X_sequences.shape}\")\n",
        "\n",
        "# Print year distribution\n",
        "year_distribution = tracking_df['Year'].value_counts().sort_index()\n",
        "print(f\"Year distribution in sequences:\")\n",
        "print(year_distribution)\n",
        "\n",
        "# Implement strict non-overlapping time-based CV\n",
        "def strict_time_cv_split(X, y_dict, tracking_df, n_splits=5):\n",
        "    \"\"\"\n",
        "    Create strictly non-overlapping time-based CV splits\n",
        "    Ensures no year appears in both training and testing sets\n",
        "    \"\"\"\n",
        "    years = sorted(tracking_df['Year'].unique())\n",
        "    total_years = len(years)\n",
        "\n",
        "    # Calculate approximate number of years for each split\n",
        "    years_per_split = total_years // n_splits\n",
        "\n",
        "    cv_splits = []\n",
        "    for i in range(n_splits):\n",
        "        # Calculate year boundaries (no overlap)\n",
        "        if i < n_splits - 1:\n",
        "            train_end_year = years[min((i+1) * years_per_split - 1, total_years-1)]\n",
        "            test_start_year = years[min((i+1) * years_per_split, total_years-1)]\n",
        "            test_end_year = years[min((i+2) * years_per_split - 1, total_years-1)]\n",
        "        else:\n",
        "            # Last fold uses all remaining years for testing\n",
        "            train_end_year = years[min((i+1) * years_per_split - 1, total_years-1)]\n",
        "            test_start_year = years[min((i+1) * years_per_split, total_years-1)]\n",
        "            test_end_year = years[-1]\n",
        "\n",
        "        # Get indices for this split\n",
        "        train_indices = tracking_df[tracking_df['Year'] <= train_end_year].index\n",
        "        test_indices = tracking_df[(tracking_df['Year'] >= test_start_year) &\n",
        "                                   (tracking_df['Year'] <= test_end_year)].index\n",
        "\n",
        "        # Create the train/test split\n",
        "        X_train = X[train_indices]\n",
        "        X_test = X[test_indices]\n",
        "        y_train = {target: y_dict[target][train_indices] for target in y_dict}\n",
        "        y_test = {target: y_dict[target][test_indices] for target in y_dict}\n",
        "\n",
        "        cv_splits.append((X_train, y_train, X_test, y_test))\n",
        "\n",
        "        # Print information about this fold\n",
        "        train_years = tracking_df.iloc[train_indices]['Year'].unique()\n",
        "        test_years = tracking_df.iloc[test_indices]['Year'].unique()\n",
        "        print(f\"Fold {i+1}: Train years: {min(train_years)}-{max(train_years)}, \"\n",
        "              f\"Test years: {min(test_years)}-{max(test_years)}, \"\n",
        "              f\"Train size: {len(train_indices)}, Test size: {len(test_indices)}\")\n",
        "\n",
        "    return cv_splits\n",
        "\n",
        "# Create CV splits with strict time-based boundaries\n",
        "print(\"\\nCreating strict time-based cross-validation splits (no overlapping years)...\")\n",
        "cv_splits = strict_time_cv_split(X_sequences, y_dict, tracking_df, n_splits=5)\n",
        "\n",
        "# Model building functions\n",
        "def build_mlp_model(input_shape, target_name):\n",
        "    \"\"\"Build a Multi-Layer Perceptron model\"\"\"\n",
        "    input_layer = Input(shape=input_shape)\n",
        "    x = Flatten()(input_layer)\n",
        "\n",
        "    # MLP layers\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    output = Dense(1, activation='linear')(x)\n",
        "\n",
        "    # Use safe name without spaces\n",
        "    model = Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def build_lstm_model(input_shape, target_name):\n",
        "    \"\"\"Build an LSTM model\"\"\"\n",
        "    input_layer = Input(shape=input_shape)\n",
        "\n",
        "    # LSTM layers\n",
        "    x = LSTM(32, return_sequences=True)(input_layer)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    x = LSTM(16)(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    output = Dense(1, activation='linear')(x)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def build_transformer_model(input_shape, target_name):\n",
        "    \"\"\"Build a simple Transformer model\"\"\"\n",
        "    input_layer = Input(shape=input_shape)\n",
        "\n",
        "    # Self-attention\n",
        "    x = input_layer\n",
        "\n",
        "    # Transformer block\n",
        "    attn = MultiHeadAttention(num_heads=4, key_dim=8)(x, x)\n",
        "    x = LayerNormalization(epsilon=1e-6)(x + attn)\n",
        "\n",
        "    # Feed-forward network\n",
        "    ffn = Dense(32, activation='relu')(x)\n",
        "    ffn = Dense(input_shape[-1])(ffn)\n",
        "    x = LayerNormalization(epsilon=1e-6)(x + ffn)\n",
        "\n",
        "    # Flatten for final dense layers\n",
        "    x = Flatten()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    output = Dense(1, activation='linear')(x)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Train and evaluate with CV\n",
        "def train_and_evaluate_cv(model_type, cv_splits, input_shape, target_name):\n",
        "    \"\"\"Train and evaluate a model using cross-validation for a specific target\"\"\"\n",
        "    all_rmse = []\n",
        "    all_models = []\n",
        "\n",
        "    for fold_idx, (X_train, y_train_dict, X_test, y_test_dict) in enumerate(cv_splits):\n",
        "        # Get target-specific data\n",
        "        y_train = y_train_dict[target_name]\n",
        "        y_test = y_test_dict[target_name]\n",
        "\n",
        "        print(f\"\\nTraining {model_type} for {target_name} - Fold {fold_idx+1}/{len(cv_splits)}\")\n",
        "\n",
        "        # Build model based on type\n",
        "        if model_type == 'mlp':\n",
        "            model = build_mlp_model(input_shape, target_name)\n",
        "        elif model_type == 'lstm':\n",
        "            model = build_lstm_model(input_shape, target_name)\n",
        "        elif model_type == 'transformer':\n",
        "            model = build_transformer_model(input_shape, target_name)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
        "\n",
        "        callbacks = [\n",
        "            EarlyStopping(patience=5, restore_best_weights=True),\n",
        "            ReduceLROnPlateau(factor=0.5, patience=3, min_lr=1e-6)\n",
        "        ]\n",
        "\n",
        "        # Train model\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_split=0.2,\n",
        "            epochs=10,  # Limited epochs for demo\n",
        "            batch_size=32,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Evaluate on test data\n",
        "        y_pred = model.predict(X_test).flatten()\n",
        "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "        all_rmse.append(rmse)\n",
        "        all_models.append(model)\n",
        "\n",
        "        print(f\"{model_type.upper()} - {target_name} - Fold {fold_idx+1} Test RMSE: {rmse:.4f}\")\n",
        "\n",
        "        # Plot loss curves for this fold\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(history.history['loss'], label='Training Loss')\n",
        "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "        plt.title(f'{model_type.upper()} Loss for {target_name} - Fold {fold_idx+1}')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss (MSE)')\n",
        "        plt.legend()\n",
        "        plt.savefig(f\"plots/{model_type}{target_name.replace(' ', '')}_fold{fold_idx+1}_loss.png\")\n",
        "        plt.close()\n",
        "\n",
        "        # Save predictions for this fold\n",
        "        results_df = pd.DataFrame({\n",
        "            'Actual': y_test,\n",
        "            'Predicted': y_pred,\n",
        "            'Error': y_test - y_pred\n",
        "        })\n",
        "        results_df.to_csv(f\"results/{model_type}{target_name.replace(' ', '')}_fold{fold_idx+1}_predictions.csv\", index=False)\n",
        "\n",
        "        # For the last fold, perform model interpretability\n",
        "        if fold_idx == len(cv_splits) - 1:\n",
        "            # Analyze feature importance\n",
        "            analyze_feature_importance_inline(\n",
        "                model_type=model_type,\n",
        "                target_name=target_name,\n",
        "                X_test=X_test,\n",
        "                y_test=y_test,\n",
        "                feature_names=feature_columns,\n",
        "                model=model\n",
        "            )\n",
        "\n",
        "            # Visualize prediction confidence\n",
        "            visualize_prediction_confidence_inline(\n",
        "                model_type=model_type,\n",
        "                target_name=target_name,\n",
        "                X_test=X_test,\n",
        "                y_test=y_test,\n",
        "                model=model\n",
        "            )\n",
        "\n",
        "    # Calculate average RMSE across folds\n",
        "    avg_rmse = np.mean(all_rmse)\n",
        "    print(f\"{model_type.upper()} - {target_name} - Average CV RMSE: {avg_rmse:.4f}\")\n",
        "\n",
        "    # Save the best model (with lowest RMSE)\n",
        "    best_model_idx = np.argmin(all_rmse)\n",
        "    best_model = all_models[best_model_idx]\n",
        "    best_model.save(f\"models/{model_type}{target_name.replace(' ', '')}_best.h5\")\n",
        "\n",
        "    return best_model, avg_rmse\n",
        "\n",
        "# Model comparison\n",
        "results = []\n",
        "\n",
        "# Train models for each target with cross-validation\n",
        "input_shape = (3, len(feature_columns))  # (sequence_length, num_features)\n",
        "\n",
        "# Store importance data for comparison across targets\n",
        "importance_data = {model_type: {} for model_type in ['mlp', 'lstm', 'transformer']}\n",
        "\n",
        "for target in target_columns:\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Training models for {target}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    for model_type in ['mlp', 'lstm', 'transformer']:\n",
        "        model, rmse = train_and_evaluate_cv(\n",
        "            model_type,\n",
        "            cv_splits,\n",
        "            input_shape,\n",
        "            target\n",
        "        )\n",
        "\n",
        "        results.append({\n",
        "            'Model': model_type,\n",
        "            'Target': target,\n",
        "            'RMSE': rmse\n",
        "        })\n",
        "\n",
        "        # Load feature importance if available\n",
        "        importance_file = f\"interpretability/{model_type}{target.replace(' ', '')}_importance.csv\"\n",
        "        if os.path.exists(importance_file):\n",
        "            imp_df = pd.read_csv(importance_file)\n",
        "            importance_data[model_type][target] = imp_df\n",
        "\n",
        "# Create comparison visualization with CV results\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(\"results/demo_model_strict_cv_comparison.csv\", index=False)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=results_df, x='Target', y='RMSE', hue='Model')\n",
        "plt.title(\"Model Performance Comparison with Strict Time-Based CV (RMSE)\")\n",
        "plt.ylabel(\"RMSE (lower is better)\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"plots/demo_model_strict_cv_comparison.png\")\n",
        "\n",
        "# Compare feature importance across targets for each model type\n",
        "print(\"\\nComparing feature importance across targets...\")\n",
        "for model_type in ['mlp', 'lstm', 'transformer']:\n",
        "    if importance_data[model_type]:\n",
        "        compare_importances(importance_data[model_type], model_type)\n",
        "\n",
        "print(\"\\nModel interpretability analysis completed. Results saved to the 'interpretability' directory.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "p1OH8R5ntmiL",
        "outputId": "0534bb98-c048-4ab5-a49a-9509f0fb16cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 13908 sequences with shape (13908, 3, 28)\n",
            "Year distribution in sequences:\n",
            "Year\n",
            "2001    444\n",
            "2002    468\n",
            "2003    492\n",
            "2004    512\n",
            "2005    539\n",
            "2006    545\n",
            "2007    575\n",
            "2008    624\n",
            "2009    674\n",
            "2010    735\n",
            "2011    746\n",
            "2012    774\n",
            "2013    813\n",
            "2014    817\n",
            "2015    824\n",
            "2016    831\n",
            "2017    865\n",
            "2018    865\n",
            "2019    882\n",
            "2020    883\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Creating strict time-based cross-validation splits (no overlapping years)...\n",
            "Fold 1: Train years: 2001-2004, Test years: 2005-2008, Train size: 1916, Test size: 2283\n",
            "Fold 2: Train years: 2001-2008, Test years: 2009-2012, Train size: 4199, Test size: 2929\n",
            "Fold 3: Train years: 2001-2012, Test years: 2013-2016, Train size: 7128, Test size: 3285\n",
            "Fold 4: Train years: 2001-2016, Test years: 2017-2020, Train size: 10413, Test size: 3495\n",
            "Fold 5: Train years: 2001-2020, Test years: 2020-2020, Train size: 13908, Test size: 883\n",
            "\n",
            "==================================================\n",
            "Training models for Target 3\n",
            "==================================================\n",
            "\n",
            "Training mlp for Target 3 - Fold 1/5\n",
            "Epoch 1/10\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - loss: 32946.8828 - mae: 151.2379 - val_loss: 32174.9297 - val_mae: 147.5871 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 32595.5879 - mae: 150.8294 - val_loss: 31711.4062 - val_mae: 147.1120 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 31964.4492 - mae: 149.9696 - val_loss: 30733.7871 - val_mae: 145.8701 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 30810.4883 - mae: 148.3692 - val_loss: 29173.8262 - val_mae: 143.6173 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 28910.8633 - mae: 145.1091 - val_loss: 27602.7949 - val_mae: 141.1724 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 27514.7402 - mae: 142.8699 - val_loss: 26985.2734 - val_mae: 140.8122 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 26287.2246 - mae: 140.2262 - val_loss: 26979.9375 - val_mae: 141.1163 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 26045.8652 - mae: 139.8534 - val_loss: 27041.4297 - val_mae: 141.9570 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 25425.3008 - mae: 138.1627 - val_loss: 27155.0215 - val_mae: 142.3707 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 25344.9551 - mae: 137.8863 - val_loss: 27114.7656 - val_mae: 142.0848 - learning_rate: 0.0010\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
            "MLP - Target 3 - Fold 1 Test RMSE: 112.6597\n",
            "\n",
            "Training mlp for Target 3 - Fold 2/5\n",
            "Epoch 1/10\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - loss: 21418.6582 - mae: 111.4400 - val_loss: 22697.5430 - val_mae: 115.3219 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 20727.9844 - mae: 110.6031 - val_loss: 21409.3457 - val_mae: 113.6921 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 19115.8906 - mae: 108.8443 - val_loss: 20222.6660 - val_mae: 112.8882 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 18089.0859 - mae: 107.2159 - val_loss: 19885.1934 - val_mae: 113.5856 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 17616.7754 - mae: 106.2021 - val_loss: 19869.1426 - val_mae: 113.8946 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 17369.9727 - mae: 105.5793 - val_loss: 19821.6152 - val_mae: 113.9301 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 17196.1895 - mae: 104.7207 - val_loss: 19811.4590 - val_mae: 113.5907 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 17028.6992 - mae: 104.3625 - val_loss: 19804.5098 - val_mae: 113.6459 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 16815.4590 - mae: 103.4879 - val_loss: 19832.6543 - val_mae: 113.9289 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 16824.3535 - mae: 103.3616 - val_loss: 19822.1211 - val_mae: 114.0090 - learning_rate: 0.0010\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "MLP - Target 3 - Fold 2 Test RMSE: 121.9592\n",
            "\n",
            "Training mlp for Target 3 - Fold 3/5\n",
            "Epoch 1/10\n",
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - loss: 17660.2129 - mae: 98.8060 - val_loss: 19155.3398 - val_mae: 105.4327 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 16673.2715 - mae: 98.9219 - val_loss: 18056.5391 - val_mae: 105.3116 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 15849.0732 - mae: 99.4667 - val_loss: 17739.2520 - val_mae: 105.6920 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 15493.9219 - mae: 99.3485 - val_loss: 17638.1328 - val_mae: 105.2829 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 15331.3291 - mae: 98.6174 - val_loss: 17564.2129 - val_mae: 105.0708 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 15204.2197 - mae: 98.1576 - val_loss: 17528.3594 - val_mae: 104.7602 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 15089.0830 - mae: 97.7962 - val_loss: 17493.2051 - val_mae: 104.8348 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 14906.3428 - mae: 97.2135 - val_loss: 17563.5352 - val_mae: 104.9661 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 14899.2393 - mae: 96.7078 - val_loss: 17593.0254 - val_mae: 104.6914 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 14725.3721 - mae: 96.4865 - val_loss: 17586.3105 - val_mae: 105.1981 - learning_rate: 0.0010\n",
            "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "MLP - Target 3 - Fold 3 Test RMSE: 107.5817\n",
            "\n",
            "Training mlp for Target 3 - Fold 4/5\n",
            "Epoch 1/10\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - loss: 16174.1006 - mae: 94.8375 - val_loss: 16567.4746 - val_mae: 98.3919 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 14971.7402 - mae: 95.3508 - val_loss: 15871.6348 - val_mae: 98.1227 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 14563.5957 - mae: 95.0402 - val_loss: 15751.8320 - val_mae: 97.8236 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 14292.5049 - mae: 94.1584 - val_loss: 15680.4531 - val_mae: 97.5636 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 14272.0820 - mae: 93.9212 - val_loss: 15636.8867 - val_mae: 97.2203 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 14105.5898 - mae: 93.4667 - val_loss: 15620.9150 - val_mae: 97.1570 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 13988.5508 - mae: 93.1109 - val_loss: 15582.4297 - val_mae: 96.9298 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 13896.7227 - mae: 92.4352 - val_loss: 15544.2734 - val_mae: 96.8730 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 13747.5898 - mae: 91.9596 - val_loss: 15564.7285 - val_mae: 96.4215 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 13701.5303 - mae: 91.9442 - val_loss: 15526.3818 - val_mae: 96.6091 - learning_rate: 0.0010\n",
            "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "MLP - Target 3 - Fold 4 Test RMSE: 125.7679\n",
            "\n",
            "Training mlp for Target 3 - Fold 5/5\n",
            "Epoch 1/10\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 16060.5850 - mae: 94.2222 - val_loss: 15983.7051 - val_mae: 98.3736 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 14857.5469 - mae: 95.5304 - val_loss: 15458.8271 - val_mae: 97.9445 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 14523.0625 - mae: 95.4099 - val_loss: 15389.0898 - val_mae: 97.8269 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 14341.6484 - mae: 94.8878 - val_loss: 15350.3525 - val_mae: 97.7603 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 14321.1738 - mae: 94.4675 - val_loss: 15322.7852 - val_mae: 97.9575 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 14203.0889 - mae: 94.1765 - val_loss: 15272.6455 - val_mae: 97.7969 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 14102.8203 - mae: 93.5410 - val_loss: 15212.3418 - val_mae: 97.7130 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 14036.0381 - mae: 93.2829 - val_loss: 15241.7871 - val_mae: 97.5049 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 13959.6475 - mae: 93.0818 - val_loss: 15198.4941 - val_mae: 97.1139 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 13730.9268 - mae: 92.3113 - val_loss: 15194.2129 - val_mae: 96.8442 - learning_rate: 0.0010\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "MLP - Target 3 - Fold 5 Test RMSE: 118.8583\n",
            "\n",
            "Analyzing feature importance for mlp on Target 3...\n",
            "\n",
            "Visualizing prediction confidence for mlp on Target 3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP - Target 3 - Average CV RMSE: 117.3654\n",
            "\n",
            "Training lstm for Target 3 - Fold 1/5\n",
            "Epoch 1/10\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 33057.1523 - mae: 151.3607 - val_loss: 32293.0449 - val_mae: 147.6634 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 32800.2266 - mae: 151.1311 - val_loss: 31911.1699 - val_mae: 147.4057 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 32448.0625 - mae: 150.8366 - val_loss: 31713.7402 - val_mae: 147.2874 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 32265.7285 - mae: 150.6079 - val_loss: 31573.7051 - val_mae: 147.1750 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 32160.1328 - mae: 150.5771 - val_loss: 31455.9688 - val_mae: 147.0943 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 32010.8613 - mae: 150.3178 - val_loss: 31353.2559 - val_mae: 147.0148 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 31950.4375 - mae: 150.3355 - val_loss: 31261.9824 - val_mae: 146.9179 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 31857.0312 - mae: 150.2298 - val_loss: 31176.5859 - val_mae: 146.8166 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 31766.1133 - mae: 150.0974 - val_loss: 31095.6094 - val_mae: 146.7247 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 31677.6602 - mae: 149.9477 - val_loss: 31020.3203 - val_mae: 146.6802 - learning_rate: 0.0010\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "LSTM - Target 3 - Fold 1 Test RMSE: 107.4037\n",
            "\n",
            "Training lstm for Target 3 - Fold 2/5\n",
            "Epoch 1/10\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 21483.6992 - mae: 111.5729 - val_loss: 22747.3984 - val_mae: 115.8879 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 21136.1211 - mae: 111.6887 - val_loss: 22453.3242 - val_mae: 115.8527 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 20932.4629 - mae: 111.7973 - val_loss: 22297.6348 - val_mae: 115.8585 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 20848.6406 - mae: 111.9620 - val_loss: 22190.9492 - val_mae: 115.8975 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 20727.3535 - mae: 111.9729 - val_loss: 22097.3438 - val_mae: 115.9576 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 20682.6250 - mae: 112.1158 - val_loss: 22022.1777 - val_mae: 116.0244 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 20583.9961 - mae: 112.0693 - val_loss: 21946.7910 - val_mae: 116.0950 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 20502.6445 - mae: 112.1105 - val_loss: 21879.2539 - val_mae: 116.0783 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 20438.7715 - mae: 111.9042 - val_loss: 21799.3711 - val_mae: 115.8501 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 20340.5547 - mae: 111.4889 - val_loss: 21735.1270 - val_mae: 115.8041 - learning_rate: 0.0010\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "LSTM - Target 3 - Fold 2 Test RMSE: 110.2666\n",
            "\n",
            "Training lstm for Target 3 - Fold 3/5\n",
            "Epoch 1/10\n",
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 17735.5215 - mae: 98.8282 - val_loss: 19527.9355 - val_mae: 105.6407 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 17416.1289 - mae: 99.0577 - val_loss: 19286.9043 - val_mae: 105.4726 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 17264.3359 - mae: 99.1013 - val_loss: 19131.5918 - val_mae: 105.3348 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 17118.7715 - mae: 99.0240 - val_loss: 18990.7031 - val_mae: 105.3279 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 17003.5605 - mae: 99.0435 - val_loss: 18865.1758 - val_mae: 105.3263 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 16879.7266 - mae: 98.9920 - val_loss: 18755.0371 - val_mae: 105.2949 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 16764.7500 - mae: 98.9366 - val_loss: 18668.5879 - val_mae: 105.3331 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 16638.6035 - mae: 98.7179 - val_loss: 18580.8340 - val_mae: 105.1689 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 16525.4512 - mae: 98.4107 - val_loss: 18532.0996 - val_mae: 104.8867 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 16426.3809 - mae: 98.0910 - val_loss: 18476.9004 - val_mae: 104.5678 - learning_rate: 0.0010\n",
            "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
            "LSTM - Target 3 - Fold 3 Test RMSE: 107.6753\n",
            "\n",
            "Training lstm for Target 3 - Fold 4/5\n",
            "Epoch 1/10\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 16287.4268 - mae: 94.9017 - val_loss: 17210.6113 - val_mae: 98.6634 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 15908.0371 - mae: 95.2421 - val_loss: 16996.9785 - val_mae: 98.4913 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 15722.5381 - mae: 95.1079 - val_loss: 16831.7930 - val_mae: 98.6147 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 15594.8291 - mae: 95.2232 - val_loss: 16721.4531 - val_mae: 98.6874 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 15468.6230 - mae: 95.2041 - val_loss: 16604.1406 - val_mae: 98.7037 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 15351.8047 - mae: 95.1695 - val_loss: 16515.9102 - val_mae: 98.3650 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 15215.4883 - mae: 94.7282 - val_loss: 16428.4297 - val_mae: 98.0118 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 15050.3672 - mae: 94.1910 - val_loss: 16336.1729 - val_mae: 97.4087 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 14923.8408 - mae: 93.7038 - val_loss: 16272.8145 - val_mae: 97.2482 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 14774.7334 - mae: 93.3322 - val_loss: 16246.1357 - val_mae: 97.1149 - learning_rate: 0.0010\n",
            "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "LSTM - Target 3 - Fold 4 Test RMSE: 126.2045\n",
            "\n",
            "Training lstm for Target 3 - Fold 5/5\n",
            "Epoch 1/10\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 16104.1406 - mae: 94.3216 - val_loss: 16732.4922 - val_mae: 98.0505 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 15679.2139 - mae: 94.7992 - val_loss: 16523.0332 - val_mae: 98.1908 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 15488.4883 - mae: 94.9262 - val_loss: 16385.7832 - val_mae: 98.3707 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 15346.2275 - mae: 95.0430 - val_loss: 16289.0146 - val_mae: 98.3688 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 15221.9023 - mae: 95.0549 - val_loss: 16174.3584 - val_mae: 98.5278 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 15086.7021 - mae: 95.0207 - val_loss: 16108.6445 - val_mae: 98.4691 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 14975.3125 - mae: 94.9236 - val_loss: 15999.5615 - val_mae: 98.4944 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 14851.1523 - mae: 94.6857 - val_loss: 15941.8936 - val_mae: 98.3091 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 14773.4365 - mae: 94.5144 - val_loss: 15884.4863 - val_mae: 98.2491 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 14634.9844 - mae: 94.2214 - val_loss: 15866.3936 - val_mae: 98.2316 - learning_rate: 0.0010\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "LSTM - Target 3 - Fold 5 Test RMSE: 129.6821\n",
            "\n",
            "Analyzing feature importance for lstm on Target 3...\n",
            "\n",
            "Visualizing prediction confidence for lstm on Target 3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM - Target 3 - Average CV RMSE: 116.2464\n",
            "\n",
            "Training transformer for Target 3 - Fold 1/5\n",
            "Epoch 1/10\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 145ms/step - loss: 32937.7461 - mae: 151.1749 - val_loss: 31755.9785 - val_mae: 147.2496 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 32210.2695 - mae: 150.5041 - val_loss: 31256.4844 - val_mae: 146.9261 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 31698.8574 - mae: 150.0027 - val_loss: 30663.8965 - val_mae: 146.4491 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 31138.8086 - mae: 149.3831 - val_loss: 30030.9688 - val_mae: 145.8785 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 30490.7090 - mae: 148.5600 - val_loss: 29368.5020 - val_mae: 145.1464 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 29908.1309 - mae: 147.8577 - val_loss: 28821.4531 - val_mae: 144.9357 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 29296.6934 - mae: 147.0833 - val_loss: 28528.9297 - val_mae: 145.3837 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 28778.0234 - mae: 146.4895 - val_loss: 28259.4766 - val_mae: 145.3869 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 28359.4570 - mae: 146.0790 - val_loss: 28057.4902 - val_mae: 145.7398 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 28107.8242 - mae: 145.9106 - val_loss: 27841.0801 - val_mae: 145.6624 - learning_rate: 0.0010\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
            "TRANSFORMER - Target 3 - Fold 1 Test RMSE: 112.9491\n",
            "\n",
            "Training transformer for Target 3 - Fold 2/5\n",
            "Epoch 1/10\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 61ms/step - loss: 21358.9102 - mae: 111.7516 - val_loss: 22176.0410 - val_mae: 115.9595 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 20571.1133 - mae: 111.9798 - val_loss: 21523.4863 - val_mae: 115.0755 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 19964.9941 - mae: 110.9121 - val_loss: 20954.0645 - val_mae: 114.1974 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 19351.8477 - mae: 109.5263 - val_loss: 20675.3887 - val_mae: 113.9217 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 18891.7246 - mae: 108.8496 - val_loss: 20598.1289 - val_mae: 113.9465 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 18571.4707 - mae: 108.3970 - val_loss: 20407.7070 - val_mae: 114.2049 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 18374.8965 - mae: 108.2308 - val_loss: 20259.3945 - val_mae: 114.2018 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 18203.3672 - mae: 107.7257 - val_loss: 20187.0840 - val_mae: 113.7646 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 17966.5547 - mae: 106.8585 - val_loss: 20231.6309 - val_mae: 113.4751 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 17723.4648 - mae: 106.0579 - val_loss: 20219.4570 - val_mae: 113.3663 - learning_rate: 0.0010\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step\n",
            "TRANSFORMER - Target 3 - Fold 2 Test RMSE: 115.9124\n",
            "\n",
            "Training transformer for Target 3 - Fold 3/5\n",
            "Epoch 1/10\n",
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - loss: 17565.5410 - mae: 99.0327 - val_loss: 18893.9395 - val_mae: 105.4928 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 16808.1406 - mae: 99.3170 - val_loss: 18297.3516 - val_mae: 104.6195 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 16278.1885 - mae: 99.2455 - val_loss: 18006.7578 - val_mae: 104.9487 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 15930.6074 - mae: 99.1009 - val_loss: 17863.7090 - val_mae: 104.9662 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 15686.5146 - mae: 98.7027 - val_loss: 17814.6992 - val_mae: 104.7528 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 15533.1436 - mae: 98.3695 - val_loss: 17744.2617 - val_mae: 104.5467 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 15376.6445 - mae: 97.9352 - val_loss: 17712.3965 - val_mae: 104.4722 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 15243.9775 - mae: 97.4515 - val_loss: 17721.4707 - val_mae: 104.5326 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 15111.9053 - mae: 97.0247 - val_loss: 17760.2930 - val_mae: 104.6293 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 14954.8105 - mae: 96.5424 - val_loss: 17740.4453 - val_mae: 104.7952 - learning_rate: 0.0010\n",
            "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
            "TRANSFORMER - Target 3 - Fold 3 Test RMSE: 106.7716\n",
            "\n",
            "Training transformer for Target 3 - Fold 4/5\n",
            "Epoch 1/10\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 20ms/step - loss: 16122.1865 - mae: 95.1262 - val_loss: 16627.6992 - val_mae: 98.2134 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 15329.7422 - mae: 95.6014 - val_loss: 16206.3350 - val_mae: 97.6093 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 14804.3223 - mae: 94.8253 - val_loss: 16109.9316 - val_mae: 97.5781 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 14600.5381 - mae: 94.4133 - val_loss: 16001.9824 - val_mae: 97.3783 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 14412.2627 - mae: 93.9848 - val_loss: 15878.7812 - val_mae: 97.4093 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 14254.0664 - mae: 93.5182 - val_loss: 15827.8877 - val_mae: 97.3497 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 14111.3564 - mae: 93.0270 - val_loss: 15769.7871 - val_mae: 97.6320 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 13988.7275 - mae: 92.6372 - val_loss: 15783.1377 - val_mae: 97.5551 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 13904.9902 - mae: 92.2279 - val_loss: 15769.3223 - val_mae: 97.5644 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 13718.0771 - mae: 91.6924 - val_loss: 15748.6143 - val_mae: 97.6349 - learning_rate: 0.0010\n",
            "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
            "TRANSFORMER - Target 3 - Fold 4 Test RMSE: 125.5681\n",
            "\n",
            "Training transformer for Target 3 - Fold 5/5\n",
            "Epoch 1/10\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - loss: 15827.5156 - mae: 94.6609 - val_loss: 16096.9434 - val_mae: 98.3125 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 15000.8164 - mae: 95.8664 - val_loss: 15805.9785 - val_mae: 98.4519 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 14709.5176 - mae: 95.9255 - val_loss: 15706.6416 - val_mae: 98.4937 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 14524.1885 - mae: 95.2638 - val_loss: 15660.4209 - val_mae: 98.6409 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 14386.8281 - mae: 94.7981 - val_loss: 15626.0664 - val_mae: 98.8000 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 14195.8887 - mae: 94.1148 - val_loss: 15618.8525 - val_mae: 98.9752 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 14068.2158 - mae: 93.6533 - val_loss: 15608.8320 - val_mae: 98.9725 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 13912.4756 - mae: 92.9947 - val_loss: 15586.2920 - val_mae: 99.1537 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 13806.3945 - mae: 92.7144 - val_loss: 15582.1758 - val_mae: 99.0033 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 13654.4727 - mae: 92.0379 - val_loss: 15599.0439 - val_mae: 98.8909 - learning_rate: 0.0010\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
            "TRANSFORMER - Target 3 - Fold 5 Test RMSE: 120.6834\n",
            "\n",
            "Analyzing feature importance for transformer on Target 3...\n",
            "\n",
            "Visualizing prediction confidence for transformer on Target 3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRANSFORMER - Target 3 - Average CV RMSE: 116.3769\n",
            "\n",
            "Comparing feature importance across targets...\n",
            "\n",
            "Model interpretability analysis completed. Results saved to the 'interpretability' directory.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## using GPU"
      ],
      "metadata": {
        "id": "lCaNmsIhuGf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Check for available GPUs\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    print(f\"Found {len(gpus)} GPU(s): {gpus}\")\n",
        "    try:\n",
        "        # Configure memory growth to avoid allocating all memory at once\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "        print(f\"{len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "        # Memory growth must be set before GPUs have been initialized\n",
        "        print(e)\n",
        "else:\n",
        "    print(\"No GPU found. Running on CPU.\")\n",
        "\n",
        "# Verify TensorFlow is using GPU\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"Is GPU available: {tf.test.is_gpu_available()}\")\n",
        "print(\"Devices available:\")\n",
        "print(tf.config.list_physical_devices())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxkqtAXGuBOg",
        "outputId": "c134a0bc-9e1e-4df9-cc78-40d942e31d0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From <ipython-input-15-6feda03b46b8>:21: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 GPU(s): [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "Physical devices cannot be modified after being initialized\n",
            "TensorFlow version: 2.18.0\n",
            "Is GPU available: True\n",
            "Devices available:\n",
            "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# GPU memory optimization\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    # Restrict TensorFlow to only allocate what's needed on the GPU\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "        print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A06sUrosuJQ_",
        "outputId": "0b7e2879-a72c-47ec-e613-957ad5a4f5fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Physical devices cannot be modified after being initialized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create directories for saving models and results\n",
        "os.makedirs('models', exist_ok=True)\n",
        "os.makedirs('results', exist_ok=True)\n",
        "os.makedirs('plots', exist_ok=True)\n",
        "os.makedirs('interpretability', exist_ok=True)  # For interpretability results\n",
        "\n",
        "# ================= MODEL INTERPRETABILITY FUNCTIONS =================\n",
        "# Analyze feature importance using a permutation approach\n",
        "def analyze_feature_importance_inline(model_type, target_name, X_test, y_test, feature_names, model=None):\n",
        "    \"\"\"\n",
        "    Analyze and visualize feature importance directly without loading saved models\n",
        "    \"\"\"\n",
        "    print(f\"\\nAnalyzing feature importance for {model_type} on {target_name}...\")\n",
        "\n",
        "    if model is None:\n",
        "        print(f\"No model provided for {model_type} on {target_name}, skipping analysis\")\n",
        "        return None\n",
        "\n",
        "    # Create baseline prediction\n",
        "    baseline_pred = model.predict(X_test, verbose=0).flatten()\n",
        "    baseline_mse = mean_squared_error(y_test, baseline_pred)\n",
        "\n",
        "    # Calculate feature importance via permutation\n",
        "    feature_importances = []\n",
        "\n",
        "    # For sequence data, analyze each feature\n",
        "    seq_length, n_features = X_test.shape[1], X_test.shape[2]\n",
        "\n",
        "    for feat_idx in range(n_features):\n",
        "        # Copy the test data\n",
        "        X_permuted = X_test.copy()\n",
        "\n",
        "        # Permute this feature across all time steps\n",
        "        for t in range(seq_length):\n",
        "            X_permuted[:, t, feat_idx] = np.random.permutation(X_permuted[:, t, feat_idx])\n",
        "\n",
        "        # Make predictions with permuted feature\n",
        "        perm_pred = model.predict(X_permuted, verbose=0).flatten()\n",
        "        perm_mse = mean_squared_error(y_test, perm_pred)\n",
        "\n",
        "        # Importance is the increase in error when feature is permuted\n",
        "        importance = perm_mse - baseline_mse\n",
        "        feature_importances.append(importance)\n",
        "\n",
        "    # Create DataFrame with feature importances\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': feature_importances\n",
        "    })\n",
        "    importance_df = importance_df.sort_values('Importance', ascending=False)\n",
        "\n",
        "    # Save to CSV\n",
        "    importance_df.to_csv(f\"interpretability/{model_type}{target_name.replace(' ', '')}_importance.csv\", index=False)\n",
        "\n",
        "    # Visualize top features\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    top_features = importance_df.head(10)\n",
        "    sns.barplot(x='Importance', y='Feature', data=top_features, palette='viridis')\n",
        "    plt.title(f\"Top 10 Important Features for {target_name} ({model_type.upper()})\")\n",
        "    plt.xlabel('Permutation Importance (higher = more important)')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"interpretability/{model_type}{target_name.replace(' ', '')}_importance.png\")\n",
        "    plt.close()\n",
        "\n",
        "    return importance_df\n",
        "\n",
        "# Visualize prediction distribution and confidence\n",
        "def visualize_prediction_confidence_inline(model_type, target_name, X_test, y_test, model=None):\n",
        "    \"\"\"\n",
        "    Visualize model predictions, errors, and confidence directly without loading saved models\n",
        "    \"\"\"\n",
        "    print(f\"\\nVisualizing prediction confidence for {model_type} on {target_name}...\")\n",
        "\n",
        "    if model is None:\n",
        "        print(f\"No model provided for {model_type} on {target_name}, skipping visualization\")\n",
        "        return\n",
        "\n",
        "    # Generate predictions\n",
        "    y_pred = model.predict(X_test, verbose=0).flatten()\n",
        "\n",
        "    # Calculate absolute error for each prediction\n",
        "    abs_errors = np.abs(y_test - y_pred)\n",
        "\n",
        "    # Create dataframe for visualization\n",
        "    pred_df = pd.DataFrame({\n",
        "        'Actual': y_test,\n",
        "        'Predicted': y_pred,\n",
        "        'AbsError': abs_errors\n",
        "    })\n",
        "\n",
        "    # Create scatter plot with error as color\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    scatter = plt.scatter(pred_df['Actual'], pred_df['Predicted'],\n",
        "                         c=pred_df['AbsError'], cmap='viridis', alpha=0.7)\n",
        "\n",
        "    # Add perfect prediction line\n",
        "    min_val = min(pred_df['Actual'].min(), pred_df['Predicted'].min())\n",
        "    max_val = max(pred_df['Actual'].max(), pred_df['Predicted'].max())\n",
        "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', label='Perfect Prediction')\n",
        "\n",
        "    plt.colorbar(scatter, label='Absolute Error')\n",
        "    plt.xlabel('Actual Values')\n",
        "    plt.ylabel('Predicted Values')\n",
        "    plt.title(f'Prediction Confidence - {model_type.upper()} for {target_name}')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"interpretability/{model_type}{target_name.replace(' ', '')}_prediction_confidence.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # Create error distribution plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.histplot(pred_df['AbsError'], kde=True)\n",
        "    plt.xlabel('Absolute Error')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title(f'Error Distribution - {model_type.upper()} for {target_name}')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"interpretability/{model_type}{target_name.replace(' ', '')}_error_distribution.png\")\n",
        "    plt.close()\n",
        "\n",
        "# Create a function to compare importances across targets\n",
        "def compare_importances(importance_data, model_type):\n",
        "    \"\"\"Compare feature importance across different targets\"\"\"\n",
        "    # If we have data for all targets, create comparison\n",
        "    if len(importance_data) >= 2:  # Need at least 2 targets to compare\n",
        "        # Get top 5 features from each target\n",
        "        all_top_features = []\n",
        "        for target, imp_df in importance_data.items():\n",
        "            all_top_features.extend(imp_df.head(5)['Feature'].tolist())\n",
        "\n",
        "        # Create unique list\n",
        "        all_top_features = list(set(all_top_features))\n",
        "\n",
        "        # Create comparison dataframe\n",
        "        comparison_df = pd.DataFrame({'Feature': all_top_features})\n",
        "\n",
        "        # Add importance for each target\n",
        "        for target, imp_df in importance_data.items():\n",
        "            # Get importance values for these features\n",
        "            target_df = imp_df[imp_df['Feature'].isin(all_top_features)]\n",
        "            # Create a mapping from feature to importance\n",
        "            importance_map = dict(zip(target_df['Feature'], target_df['Importance']))\n",
        "\n",
        "            # Add to comparison df with the target name as column\n",
        "            comparison_df[target] = comparison_df['Feature'].map(importance_map).fillna(0)\n",
        "\n",
        "        # Save to CSV\n",
        "        comparison_df.to_csv(f\"interpretability/{model_type}_feature_importance_comparison.csv\", index=False)\n",
        "\n",
        "        # Create heatmap visualization\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        heatmap_df = comparison_df.set_index('Feature')\n",
        "        sns.heatmap(heatmap_df, annot=True, cmap='viridis', fmt='.3f')\n",
        "        plt.title(f'Feature Importance Comparison Across Targets - {model_type.upper()}')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"interpretability/{model_type}_feature_importance_heatmap.png\")\n",
        "        plt.close()\n",
        "\n",
        "# # Load the preprocessed dataset\n",
        "# data = pd.read_csv('processed_data.csv')\n",
        "# print(f\"Dataset loaded with shape: {data.shape}\")\n",
        "\n",
        "data = df3\n",
        "\n",
        "# # Define feature and target columns\n",
        "feature_columns = [f'Feature{i}' for i in range(1, 29)]\n",
        "target_columns = ['Target 3']\n",
        "\n",
        "# Sort data by Year and Company\n",
        "data = data.sort_values(['Company', 'Year'])\n",
        "\n",
        "# Prepare sequences for time-series modeling\n",
        "def prepare_sequences(df, feature_cols, target_cols, seq_length=3):\n",
        "    \"\"\"Prepare time series sequences for each company\"\"\"\n",
        "    companies = df['Company'].unique()\n",
        "    X_sequences = []\n",
        "    y_dict = {target: [] for target in target_cols}\n",
        "    companies_included = []\n",
        "    years_included = []\n",
        "\n",
        "    for company in companies:\n",
        "        company_data = df[df['Company'] == company]\n",
        "        if len(company_data) >= seq_length + 1:  # +1 because we need at least one target\n",
        "            company_X = company_data[feature_cols].values\n",
        "            company_years = company_data['Year'].values\n",
        "\n",
        "            for i in range(len(company_data) - seq_length):\n",
        "                X_sequences.append(company_X[i:i+seq_length])\n",
        "                companies_included.append(company)\n",
        "                years_included.append(company_years[i+seq_length-1])\n",
        "\n",
        "                for target in target_cols:\n",
        "                    y_dict[target].append(company_data[target].values[i+seq_length])\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    X_sequences = np.array(X_sequences)\n",
        "    for target in target_cols:\n",
        "        y_dict[target] = np.array(y_dict[target])\n",
        "\n",
        "    # Create tracking DataFrame for analysis\n",
        "    tracking_df = pd.DataFrame({\n",
        "        'Company': companies_included,\n",
        "        'Year': years_included\n",
        "    })\n",
        "\n",
        "    return X_sequences, y_dict, tracking_df\n",
        "\n",
        "# Create sequences\n",
        "X_sequences, y_dict, tracking_df = prepare_sequences(data, feature_columns, target_columns, seq_length=3)\n",
        "print(f\"Created {len(X_sequences)} sequences with shape {X_sequences.shape}\")\n",
        "\n",
        "# Print year distribution\n",
        "year_distribution = tracking_df['Year'].value_counts().sort_index()\n",
        "print(f\"Year distribution in sequences:\")\n",
        "print(year_distribution)\n",
        "\n",
        "# Implement strict non-overlapping time-based CV\n",
        "def strict_time_cv_split(X, y_dict, tracking_df, n_splits=5):\n",
        "    \"\"\"\n",
        "    Create strictly non-overlapping time-based CV splits\n",
        "    Ensures no year appears in both training and testing sets\n",
        "    \"\"\"\n",
        "    years = sorted(tracking_df['Year'].unique())\n",
        "    total_years = len(years)\n",
        "\n",
        "    # Calculate approximate number of years for each split\n",
        "    years_per_split = total_years // n_splits\n",
        "\n",
        "    cv_splits = []\n",
        "    for i in range(n_splits):\n",
        "        # Calculate year boundaries (no overlap)\n",
        "        if i < n_splits - 1:\n",
        "            train_end_year = years[min((i+1) * years_per_split - 1, total_years-1)]\n",
        "            test_start_year = years[min((i+1) * years_per_split, total_years-1)]\n",
        "            test_end_year = years[min((i+2) * years_per_split - 1, total_years-1)]\n",
        "        else:\n",
        "            # Last fold uses all remaining years for testing\n",
        "            train_end_year = years[min((i+1) * years_per_split - 1, total_years-1)]\n",
        "            test_start_year = years[min((i+1) * years_per_split, total_years-1)]\n",
        "            test_end_year = years[-1]\n",
        "\n",
        "        # Get indices for this split\n",
        "        train_indices = tracking_df[tracking_df['Year'] <= train_end_year].index\n",
        "        test_indices = tracking_df[(tracking_df['Year'] >= test_start_year) &\n",
        "                                   (tracking_df['Year'] <= test_end_year)].index\n",
        "\n",
        "        # Create the train/test split\n",
        "        X_train = X[train_indices]\n",
        "        X_test = X[test_indices]\n",
        "        y_train = {target: y_dict[target][train_indices] for target in y_dict}\n",
        "        y_test = {target: y_dict[target][test_indices] for target in y_dict}\n",
        "\n",
        "        cv_splits.append((X_train, y_train, X_test, y_test))\n",
        "\n",
        "        # Print information about this fold\n",
        "        train_years = tracking_df.iloc[train_indices]['Year'].unique()\n",
        "        test_years = tracking_df.iloc[test_indices]['Year'].unique()\n",
        "        print(f\"Fold {i+1}: Train years: {min(train_years)}-{max(train_years)}, \"\n",
        "              f\"Test years: {min(test_years)}-{max(test_years)}, \"\n",
        "              f\"Train size: {len(train_indices)}, Test size: {len(test_indices)}\")\n",
        "\n",
        "    return cv_splits\n",
        "\n",
        "# Create CV splits with strict time-based boundaries\n",
        "print(\"\\nCreating strict time-based cross-validation splits (no overlapping years)...\")\n",
        "cv_splits = strict_time_cv_split(X_sequences, y_dict, tracking_df, n_splits=5)\n",
        "\n",
        "# Model building functions\n",
        "def build_mlp_model(input_shape, target_name):\n",
        "    \"\"\"Build a Multi-Layer Perceptron model\"\"\"\n",
        "    input_layer = Input(shape=input_shape)\n",
        "    x = Flatten()(input_layer)\n",
        "\n",
        "    # MLP layers\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    output = Dense(1, activation='linear')(x)\n",
        "\n",
        "    # Use safe name without spaces\n",
        "    model = Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def build_lstm_model(input_shape, target_name):\n",
        "    \"\"\"Build an LSTM model optimized for GPU\"\"\"\n",
        "    input_layer = Input(shape=input_shape)\n",
        "\n",
        "    # LSTM layers with GPU optimizations\n",
        "    x = LSTM(32, return_sequences=True,\n",
        "             recurrent_activation='sigmoid',  # Faster on GPU than 'hard_sigmoid'\n",
        "             implementation=2)(input_layer)  # Implementation 2 is more GPU-friendly\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    x = LSTM(16, implementation=2)(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    output = Dense(1, activation='linear')(x)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output)\n",
        "\n",
        "    # Use Adam optimizer with GPU-friendly parameters\n",
        "    optimizer = Adam(learning_rate=0.001, epsilon=1e-7)\n",
        "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def build_transformer_model(input_shape, target_name):\n",
        "    \"\"\"Build a simple Transformer model\"\"\"\n",
        "    input_layer = Input(shape=input_shape)\n",
        "\n",
        "    # Self-attention\n",
        "    x = input_layer\n",
        "\n",
        "    # Transformer block\n",
        "    attn = MultiHeadAttention(num_heads=4, key_dim=8)(x, x)\n",
        "    x = LayerNormalization(epsilon=1e-6)(x + attn)\n",
        "\n",
        "    # Feed-forward network\n",
        "    ffn = Dense(32, activation='relu')(x)\n",
        "    ffn = Dense(input_shape[-1])(ffn)\n",
        "    x = LayerNormalization(epsilon=1e-6)(x + ffn)\n",
        "\n",
        "    # Flatten for final dense layers\n",
        "    x = Flatten()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    output = Dense(1, activation='linear')(x)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Train and evaluate with CV\n",
        "def train_and_evaluate_cv(model_type, cv_splits, input_shape, target_name):\n",
        "    \"\"\"Train and evaluate a model using cross-validation for a specific target\"\"\"\n",
        "    all_rmse = []\n",
        "    all_models = []\n",
        "\n",
        "    # Use mixed precision for faster GPU training\n",
        "    if gpus:\n",
        "        tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "        print(\"Using mixed precision for GPU training\")\n",
        "\n",
        "    for fold_idx, (X_train, y_train_dict, X_test, y_test_dict) in enumerate(cv_splits):\n",
        "        # Get target-specific data\n",
        "        y_train = y_train_dict[target_name]\n",
        "        y_test = y_test_dict[target_name]\n",
        "\n",
        "        print(f\"\\nTraining {model_type} for {target_name} - Fold {fold_idx+1}/{len(cv_splits)}\")\n",
        "\n",
        "        # Build model based on type\n",
        "        if model_type == 'mlp':\n",
        "            model = build_mlp_model(input_shape, target_name)\n",
        "        elif model_type == 'lstm':\n",
        "            model = build_lstm_model(input_shape, target_name)\n",
        "        elif model_type == 'transformer':\n",
        "            model = build_transformer_model(input_shape, target_name)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
        "\n",
        "        callbacks = [\n",
        "            EarlyStopping(patience=5, restore_best_weights=True),\n",
        "            ReduceLROnPlateau(factor=0.5, patience=3, min_lr=1e-6)\n",
        "        ]\n",
        "\n",
        "        # Train model with GPU-optimized batch size\n",
        "        batch_size = 64 if gpus else 32  # Larger batch size for GPU\n",
        "\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_split=0.2,\n",
        "            epochs=10,  # Limited epochs for demo\n",
        "            batch_size=batch_size,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Evaluate on test data\n",
        "        y_pred = model.predict(X_test).flatten()\n",
        "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "        all_rmse.append(rmse)\n",
        "        all_models.append(model)\n",
        "\n",
        "        print(f\"{model_type.upper()} - {target_name} - Fold {fold_idx+1} Test RMSE: {rmse:.4f}\")\n",
        "\n",
        "        # Plot loss curves for this fold\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(history.history['loss'], label='Training Loss')\n",
        "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "        plt.title(f'{model_type.upper()} Loss for {target_name} - Fold {fold_idx+1}')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss (MSE)')\n",
        "        plt.legend()\n",
        "        plt.savefig(f\"plots/{model_type}{target_name.replace(' ', '')}_fold{fold_idx+1}_loss.png\")\n",
        "        plt.close()\n",
        "\n",
        "        # Save predictions for this fold\n",
        "        results_df = pd.DataFrame({\n",
        "            'Actual': y_test,\n",
        "            'Predicted': y_pred,\n",
        "            'Error': y_test - y_pred\n",
        "        })\n",
        "        results_df.to_csv(f\"results/{model_type}{target_name.replace(' ', '')}_fold{fold_idx+1}_predictions.csv\", index=False)\n",
        "\n",
        "        # For the last fold, perform model interpretability\n",
        "        if fold_idx == len(cv_splits) - 1:\n",
        "            # Analyze feature importance\n",
        "            analyze_feature_importance_inline(\n",
        "                model_type=model_type,\n",
        "                target_name=target_name,\n",
        "                X_test=X_test,\n",
        "                y_test=y_test,\n",
        "                feature_names=feature_columns,\n",
        "                model=model\n",
        "            )\n",
        "\n",
        "            # Visualize prediction confidence\n",
        "            visualize_prediction_confidence_inline(\n",
        "                model_type=model_type,\n",
        "                target_name=target_name,\n",
        "                X_test=X_test,\n",
        "                y_test=y_test,\n",
        "                model=model\n",
        "            )\n",
        "\n",
        "    # Calculate average RMSE across folds\n",
        "    avg_rmse = np.mean(all_rmse)\n",
        "    print(f\"{model_type.upper()} - {target_name} - Average CV RMSE: {avg_rmse:.4f}\")\n",
        "\n",
        "    # Save the best model (with lowest RMSE)\n",
        "    best_model_idx = np.argmin(all_rmse)\n",
        "    best_model = all_models[best_model_idx]\n",
        "    best_model.save(f\"models/{model_type}{target_name.replace(' ', '')}_best.h5\")\n",
        "\n",
        "    return best_model, avg_rmse\n",
        "\n",
        "# Model comparison\n",
        "results = []\n",
        "\n",
        "# Train models for each target with cross-validation\n",
        "input_shape = (3, len(feature_columns))  # (sequence_length, num_features)\n",
        "\n",
        "# Store importance data for comparison across targets\n",
        "importance_data = {model_type: {} for model_type in ['mlp', 'lstm', 'transformer']}\n",
        "\n",
        "for target in target_columns:\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Training models for {target}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    for model_type in ['mlp', 'lstm', 'transformer']:\n",
        "        model, rmse = train_and_evaluate_cv(\n",
        "            model_type,\n",
        "            cv_splits,\n",
        "            input_shape,\n",
        "            target\n",
        "        )\n",
        "\n",
        "        results.append({\n",
        "            'Model': model_type,\n",
        "            'Target': target,\n",
        "            'RMSE': rmse\n",
        "        })\n",
        "\n",
        "        # Load feature importance if available\n",
        "        importance_file = f\"interpretability/{model_type}{target.replace(' ', '')}_importance.csv\"\n",
        "        if os.path.exists(importance_file):\n",
        "            imp_df = pd.read_csv(importance_file)\n",
        "            importance_data[model_type][target] = imp_df\n",
        "\n",
        "# Create comparison visualization with CV results\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(\"results/demo_model_strict_cv_comparison.csv\", index=False)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=results_df, x='Target', y='RMSE', hue='Model')\n",
        "plt.title(\"Model Performance Comparison with Strict Time-Based CV (RMSE)\")\n",
        "plt.ylabel(\"RMSE (lower is better)\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"plots/demo_model_strict_cv_comparison.png\")\n",
        "\n",
        "# Compare feature importance across targets for each model type\n",
        "print(\"\\nComparing feature importance across targets...\")\n",
        "for model_type in ['mlp', 'lstm', 'transformer']:\n",
        "    if importance_data[model_type]:\n",
        "        compare_importances(importance_data[model_type], model_type)\n",
        "\n",
        "print(\"\\nModel interpretability analysis completed. Results saved to the 'interpretability' directory.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5F8PCy5nuPgS",
        "outputId": "32da5c2e-0f01-46d2-e29a-d2c3cb12fe6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 13908 sequences with shape (13908, 3, 28)\n",
            "Year distribution in sequences:\n",
            "Year\n",
            "2001    444\n",
            "2002    468\n",
            "2003    492\n",
            "2004    512\n",
            "2005    539\n",
            "2006    545\n",
            "2007    575\n",
            "2008    624\n",
            "2009    674\n",
            "2010    735\n",
            "2011    746\n",
            "2012    774\n",
            "2013    813\n",
            "2014    817\n",
            "2015    824\n",
            "2016    831\n",
            "2017    865\n",
            "2018    865\n",
            "2019    882\n",
            "2020    883\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Creating strict time-based cross-validation splits (no overlapping years)...\n",
            "Fold 1: Train years: 2001-2004, Test years: 2005-2008, Train size: 1916, Test size: 2283\n",
            "Fold 2: Train years: 2001-2008, Test years: 2009-2012, Train size: 4199, Test size: 2929\n",
            "Fold 3: Train years: 2001-2012, Test years: 2013-2016, Train size: 7128, Test size: 3285\n",
            "Fold 4: Train years: 2001-2016, Test years: 2017-2020, Train size: 10413, Test size: 3495\n",
            "Fold 5: Train years: 2001-2020, Test years: 2020-2020, Train size: 13908, Test size: 883\n",
            "\n",
            "==================================================\n",
            "Training models for Target 3\n",
            "==================================================\n",
            "Using mixed precision for GPU training\n",
            "\n",
            "Training mlp for Target 3 - Fold 1/5\n",
            "Epoch 1/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 106ms/step - loss: 33247.0781 - mae: 151.7953 - val_loss: 32322.3984 - val_mae: 147.5931 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 33021.9922 - mae: 151.5041 - val_loss: 32081.0566 - val_mae: 147.2891 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 32731.4199 - mae: 151.0471 - val_loss: 31702.3906 - val_mae: 146.8338 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 32365.8574 - mae: 150.5221 - val_loss: 31201.7500 - val_mae: 146.1983 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 31816.3828 - mae: 149.7095 - val_loss: 30415.8750 - val_mae: 145.1644 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 31032.1895 - mae: 148.4995 - val_loss: 29543.8594 - val_mae: 143.9515 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 30018.3027 - mae: 147.1514 - val_loss: 28614.6016 - val_mae: 142.6615 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 28985.6641 - mae: 145.4459 - val_loss: 27720.2754 - val_mae: 141.3913 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 28014.2812 - mae: 143.6418 - val_loss: 27358.4707 - val_mae: 140.8731 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 26959.3301 - mae: 141.3149 - val_loss: 27065.9629 - val_mae: 140.7691 - learning_rate: 0.0010\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
            "MLP - Target 3 - Fold 1 Test RMSE: 112.0291\n",
            "\n",
            "Training mlp for Target 3 - Fold 2/5\n",
            "Epoch 1/10\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - loss: 21506.0020 - mae: 111.5980 - val_loss: 22883.1289 - val_mae: 115.6353 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 21149.6504 - mae: 111.1546 - val_loss: 22467.4961 - val_mae: 115.2134 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 20598.9453 - mae: 110.5171 - val_loss: 21694.7383 - val_mae: 114.2981 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 19579.5410 - mae: 109.3152 - val_loss: 20742.5352 - val_mae: 113.0870 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 18478.6660 - mae: 107.6444 - val_loss: 20173.4062 - val_mae: 113.1551 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 17904.2168 - mae: 106.8808 - val_loss: 20033.6758 - val_mae: 113.6332 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 17739.8125 - mae: 106.6788 - val_loss: 19953.2207 - val_mae: 113.6703 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 17249.9297 - mae: 105.4089 - val_loss: 19934.8047 - val_mae: 113.8477 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 17326.2773 - mae: 105.3926 - val_loss: 19863.6895 - val_mae: 113.8501 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 17072.0625 - mae: 104.4887 - val_loss: 19896.1270 - val_mae: 114.4589 - learning_rate: 0.0010\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "MLP - Target 3 - Fold 2 Test RMSE: 120.0292\n",
            "\n",
            "Training mlp for Target 3 - Fold 3/5\n",
            "Epoch 1/10\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 34ms/step - loss: 17740.1172 - mae: 98.8103 - val_loss: 19639.5449 - val_mae: 105.5723 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 17332.3906 - mae: 98.8679 - val_loss: 18894.1875 - val_mae: 105.4852 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 16505.3926 - mae: 99.0301 - val_loss: 18131.2012 - val_mae: 105.2944 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15812.4541 - mae: 99.3242 - val_loss: 17761.2344 - val_mae: 105.4408 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 15472.0371 - mae: 99.0118 - val_loss: 17620.8086 - val_mae: 105.7038 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 15352.0732 - mae: 98.8708 - val_loss: 17530.1758 - val_mae: 105.1428 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 15191.0254 - mae: 98.0965 - val_loss: 17480.1367 - val_mae: 105.4662 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 15061.0312 - mae: 97.8660 - val_loss: 17430.9590 - val_mae: 104.9985 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 14820.7568 - mae: 97.4322 - val_loss: 17424.2949 - val_mae: 104.9640 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 14845.8428 - mae: 97.0243 - val_loss: 17437.1172 - val_mae: 104.7407 - learning_rate: 0.0010\n",
            "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
            "MLP - Target 3 - Fold 3 Test RMSE: 107.4972\n",
            "\n",
            "Training mlp for Target 3 - Fold 4/5\n",
            "Epoch 1/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 30ms/step - loss: 16292.8584 - mae: 94.7604 - val_loss: 17249.5254 - val_mae: 98.3612 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 15620.8252 - mae: 94.9843 - val_loss: 16293.7363 - val_mae: 98.1359 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 14751.4092 - mae: 95.1086 - val_loss: 15849.6934 - val_mae: 98.0143 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 14491.5439 - mae: 94.9010 - val_loss: 15721.3398 - val_mae: 97.6368 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 14351.3535 - mae: 94.3313 - val_loss: 15637.2324 - val_mae: 97.5091 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 14131.9023 - mae: 93.5753 - val_loss: 15579.6084 - val_mae: 97.4467 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 14069.8174 - mae: 93.3658 - val_loss: 15546.5996 - val_mae: 97.2683 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 13992.5186 - mae: 92.9844 - val_loss: 15516.5361 - val_mae: 97.1962 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 13857.7168 - mae: 92.4669 - val_loss: 15499.4707 - val_mae: 96.7464 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 13886.5059 - mae: 92.4769 - val_loss: 15445.0684 - val_mae: 96.6463 - learning_rate: 0.0010\n",
            "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "MLP - Target 3 - Fold 4 Test RMSE: 124.6187\n",
            "\n",
            "Training mlp for Target 3 - Fold 5/5\n",
            "Epoch 1/10\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - loss: 15955.6670 - mae: 94.2884 - val_loss: 16160.5059 - val_mae: 98.4772 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 14926.1953 - mae: 95.2842 - val_loss: 15588.2871 - val_mae: 98.5254 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 14537.0537 - mae: 95.4270 - val_loss: 15460.7002 - val_mae: 98.2755 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 14403.7422 - mae: 94.9328 - val_loss: 15400.3984 - val_mae: 98.3028 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 14294.5635 - mae: 94.7220 - val_loss: 15371.7402 - val_mae: 98.0560 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 14278.0283 - mae: 94.6358 - val_loss: 15332.8623 - val_mae: 97.8381 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 14169.8535 - mae: 94.0956 - val_loss: 15306.5518 - val_mae: 97.8687 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 14074.0439 - mae: 93.8581 - val_loss: 15295.3418 - val_mae: 97.4771 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 14033.1572 - mae: 93.3453 - val_loss: 15275.1318 - val_mae: 97.4078 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 13892.8857 - mae: 92.8890 - val_loss: 15240.5303 - val_mae: 97.2763 - learning_rate: 0.0010\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
            "MLP - Target 3 - Fold 5 Test RMSE: 119.1895\n",
            "\n",
            "Analyzing feature importance for mlp on Target 3...\n",
            "\n",
            "Visualizing prediction confidence for mlp on Target 3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP - Target 3 - Average CV RMSE: 116.6727\n",
            "Using mixed precision for GPU training\n",
            "\n",
            "Training lstm for Target 3 - Fold 1/5\n",
            "Epoch 1/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 87ms/step - loss: 33289.0195 - mae: 151.8445 - val_loss: 32427.5723 - val_mae: 147.7455 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 33233.5938 - mae: 151.7861 - val_loss: 32316.6270 - val_mae: 147.6473 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 33079.2734 - mae: 151.6309 - val_loss: 32027.1172 - val_mae: 147.4387 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 32785.6797 - mae: 151.3815 - val_loss: 31843.8770 - val_mae: 147.3360 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 32618.4414 - mae: 151.2221 - val_loss: 31747.8047 - val_mae: 147.2746 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 32520.4707 - mae: 151.1019 - val_loss: 31666.5215 - val_mae: 147.2239 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 32445.5879 - mae: 151.0215 - val_loss: 31594.9277 - val_mae: 147.1658 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 32388.9609 - mae: 150.9444 - val_loss: 31529.0137 - val_mae: 147.0987 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 32312.0352 - mae: 150.8471 - val_loss: 31470.1660 - val_mae: 147.0343 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 32258.5430 - mae: 150.8009 - val_loss: 31414.5449 - val_mae: 146.9672 - learning_rate: 0.0010\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
            "LSTM - Target 3 - Fold 1 Test RMSE: 107.7599\n",
            "\n",
            "Training lstm for Target 3 - Fold 2/5\n",
            "Epoch 1/10\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 21519.3418 - mae: 111.6448 - val_loss: 22951.6992 - val_mae: 115.8626 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 21365.2129 - mae: 111.7426 - val_loss: 22669.7969 - val_mae: 115.9143 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 21119.9219 - mae: 111.8458 - val_loss: 22496.9785 - val_mae: 115.8474 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 20990.5000 - mae: 111.8488 - val_loss: 22392.3281 - val_mae: 115.8305 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 20912.4512 - mae: 111.8419 - val_loss: 22315.2500 - val_mae: 115.8555 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 20878.2520 - mae: 111.9992 - val_loss: 22250.2930 - val_mae: 115.8631 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 20803.2227 - mae: 111.9812 - val_loss: 22190.5391 - val_mae: 115.8872 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 20740.0430 - mae: 111.9860 - val_loss: 22135.2402 - val_mae: 115.9035 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 20723.5312 - mae: 112.0842 - val_loss: 22079.4121 - val_mae: 115.7873 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 20642.8262 - mae: 111.7082 - val_loss: 22023.8477 - val_mae: 115.7999 - learning_rate: 0.0010\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "LSTM - Target 3 - Fold 2 Test RMSE: 110.4799\n",
            "\n",
            "Training lstm for Target 3 - Fold 3/5\n",
            "Epoch 1/10\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 17764.4629 - mae: 98.8109 - val_loss: 19695.3555 - val_mae: 105.6674 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 17534.9902 - mae: 98.9967 - val_loss: 19431.7344 - val_mae: 105.6367 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 17367.9355 - mae: 99.1506 - val_loss: 19303.0820 - val_mae: 105.4187 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 17253.2422 - mae: 98.9389 - val_loss: 19196.9883 - val_mae: 105.3882 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 17182.3340 - mae: 99.1031 - val_loss: 19101.3105 - val_mae: 105.3288 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 17086.3574 - mae: 99.0474 - val_loss: 19012.2578 - val_mae: 105.2718 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 17033.1875 - mae: 99.0991 - val_loss: 18938.5723 - val_mae: 105.2714 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 16953.0508 - mae: 99.0708 - val_loss: 18864.4453 - val_mae: 105.2498 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 16878.2734 - mae: 99.0383 - val_loss: 18798.5430 - val_mae: 105.1443 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 16797.5430 - mae: 98.7796 - val_loss: 18727.7578 - val_mae: 105.0965 - learning_rate: 0.0010\n",
            "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "LSTM - Target 3 - Fold 3 Test RMSE: 107.8111\n",
            "\n",
            "Training lstm for Target 3 - Fold 4/5\n",
            "Epoch 1/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 16313.7285 - mae: 94.8448 - val_loss: 17335.2559 - val_mae: 98.5747 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 16022.7744 - mae: 95.1933 - val_loss: 17133.8320 - val_mae: 98.7150 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 15869.5986 - mae: 95.2871 - val_loss: 17004.1172 - val_mae: 98.4895 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 15754.2256 - mae: 95.1382 - val_loss: 16907.3086 - val_mae: 98.4627 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 15659.3613 - mae: 95.1142 - val_loss: 16832.7188 - val_mae: 98.5633 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 15572.8604 - mae: 95.0836 - val_loss: 16747.5840 - val_mae: 98.5622 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 15482.3818 - mae: 95.0197 - val_loss: 16688.3730 - val_mae: 98.5861 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 15393.9746 - mae: 94.9408 - val_loss: 16615.9355 - val_mae: 98.5390 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 15324.0723 - mae: 94.8717 - val_loss: 16552.9980 - val_mae: 98.3624 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 15215.3896 - mae: 94.5020 - val_loss: 16493.7910 - val_mae: 98.0932 - learning_rate: 0.0010\n",
            "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "LSTM - Target 3 - Fold 4 Test RMSE: 126.2347\n",
            "\n",
            "Training lstm for Target 3 - Fold 5/5\n",
            "Epoch 1/10\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - loss: 16142.8877 - mae: 94.2143 - val_loss: 16856.6309 - val_mae: 98.0597 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 15778.4180 - mae: 94.7415 - val_loss: 16684.6191 - val_mae: 98.1955 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 15641.4268 - mae: 94.8430 - val_loss: 16563.4746 - val_mae: 98.2795 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 15520.4023 - mae: 94.8805 - val_loss: 16458.9160 - val_mae: 98.3422 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 15421.9746 - mae: 94.9203 - val_loss: 16389.2617 - val_mae: 98.4540 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 15338.4580 - mae: 95.0105 - val_loss: 16322.9775 - val_mae: 98.5654 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 15257.7441 - mae: 95.0803 - val_loss: 16255.5928 - val_mae: 98.6938 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 15191.4873 - mae: 95.0832 - val_loss: 16177.5078 - val_mae: 98.6350 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 15091.8379 - mae: 94.9541 - val_loss: 16146.3633 - val_mae: 98.1717 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 15024.8057 - mae: 94.6363 - val_loss: 16058.3477 - val_mae: 98.1671 - learning_rate: 0.0010\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "LSTM - Target 3 - Fold 5 Test RMSE: 133.1150\n",
            "\n",
            "Analyzing feature importance for lstm on Target 3...\n",
            "\n",
            "Visualizing prediction confidence for lstm on Target 3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM - Target 3 - Average CV RMSE: 117.0801\n",
            "Using mixed precision for GPU training\n",
            "\n",
            "Training transformer for Target 3 - Fold 1/5\n",
            "Epoch 1/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 247ms/step - loss: 33172.5117 - mae: 151.7072 - val_loss: 32013.3379 - val_mae: 147.3533 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 32743.8047 - mae: 151.3202 - val_loss: 31646.5703 - val_mae: 147.1458 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 32341.2754 - mae: 150.9146 - val_loss: 31339.3203 - val_mae: 146.9513 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 32024.9512 - mae: 150.5871 - val_loss: 31020.8125 - val_mae: 146.6999 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 31722.0000 - mae: 150.2746 - val_loss: 30699.2246 - val_mae: 146.4460 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 31417.0566 - mae: 149.9189 - val_loss: 30362.3125 - val_mae: 146.1449 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 31033.2363 - mae: 149.3542 - val_loss: 30041.7969 - val_mae: 145.9147 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 30680.4043 - mae: 148.9461 - val_loss: 29705.8652 - val_mae: 145.6684 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 30331.5293 - mae: 148.4731 - val_loss: 29406.2129 - val_mae: 145.5452 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 30003.8418 - mae: 148.1081 - val_loss: 29107.1562 - val_mae: 145.4557 - learning_rate: 0.0010\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step\n",
            "TRANSFORMER - Target 3 - Fold 1 Test RMSE: 108.8325\n",
            "\n",
            "Training transformer for Target 3 - Fold 2/5\n",
            "Epoch 1/10\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - loss: 21429.6055 - mae: 111.7066 - val_loss: 22571.3789 - val_mae: 115.8176 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 20982.1523 - mae: 111.7836 - val_loss: 22147.4980 - val_mae: 115.8348 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 20629.4277 - mae: 111.9353 - val_loss: 21746.6152 - val_mae: 115.5467 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20200.7539 - mae: 111.1344 - val_loss: 21315.1191 - val_mae: 115.0080 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 19738.6953 - mae: 110.1458 - val_loss: 20928.0859 - val_mae: 115.0185 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 19287.4570 - mae: 109.4404 - val_loss: 20623.5312 - val_mae: 114.7718 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 18981.6953 - mae: 109.0251 - val_loss: 20433.0723 - val_mae: 114.9356 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 18741.6855 - mae: 108.8640 - val_loss: 20306.9707 - val_mae: 115.0658 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 18536.7676 - mae: 108.5947 - val_loss: 20235.7754 - val_mae: 115.0734 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 18367.0566 - mae: 108.2751 - val_loss: 20186.5840 - val_mae: 114.9508 - learning_rate: 0.0010\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
            "TRANSFORMER - Target 3 - Fold 2 Test RMSE: 114.9790\n",
            "\n",
            "Training transformer for Target 3 - Fold 3/5\n",
            "Epoch 1/10\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 74ms/step - loss: 17683.9453 - mae: 98.8431 - val_loss: 19347.3555 - val_mae: 105.7369 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 17189.5391 - mae: 99.2607 - val_loss: 18793.4551 - val_mae: 105.3587 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 16728.1641 - mae: 99.1410 - val_loss: 18352.8789 - val_mae: 105.1068 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 16343.6973 - mae: 99.1305 - val_loss: 18059.8164 - val_mae: 104.8987 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 16043.9893 - mae: 98.9545 - val_loss: 17896.3301 - val_mae: 104.7794 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 15830.6973 - mae: 98.6791 - val_loss: 17828.7402 - val_mae: 104.7780 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 15634.1465 - mae: 98.3734 - val_loss: 17777.1953 - val_mae: 104.7892 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 15491.0176 - mae: 98.1984 - val_loss: 17775.1660 - val_mae: 104.5950 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 15340.0322 - mae: 97.6315 - val_loss: 17729.5371 - val_mae: 104.6230 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 15180.9678 - mae: 97.0990 - val_loss: 17727.0781 - val_mae: 104.4510 - learning_rate: 0.0010\n",
            "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step\n",
            "TRANSFORMER - Target 3 - Fold 3 Test RMSE: 106.0976\n",
            "\n",
            "Training transformer for Target 3 - Fold 4/5\n",
            "Epoch 1/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 43ms/step - loss: 16203.4102 - mae: 94.9137 - val_loss: 16944.5371 - val_mae: 98.7231 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 15608.8867 - mae: 95.6013 - val_loss: 16354.3350 - val_mae: 98.0682 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 15125.3799 - mae: 95.3017 - val_loss: 16090.1826 - val_mae: 97.7914 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 14821.8613 - mae: 94.7594 - val_loss: 16018.6465 - val_mae: 97.7187 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 14646.2275 - mae: 94.3354 - val_loss: 15927.0068 - val_mae: 97.7950 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 14451.9355 - mae: 93.9427 - val_loss: 15838.4102 - val_mae: 97.9078 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 14304.9375 - mae: 93.6861 - val_loss: 15783.7949 - val_mae: 97.8495 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 14189.2549 - mae: 93.2422 - val_loss: 15729.5039 - val_mae: 97.8366 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 14052.8486 - mae: 92.7350 - val_loss: 15688.3760 - val_mae: 97.6499 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 13931.5098 - mae: 92.1991 - val_loss: 15672.2588 - val_mae: 97.6020 - learning_rate: 0.0010\n",
            "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
            "TRANSFORMER - Target 3 - Fold 4 Test RMSE: 125.7993\n",
            "\n",
            "Training transformer for Target 3 - Fold 5/5\n",
            "Epoch 1/10\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - loss: 15996.3320 - mae: 94.3457 - val_loss: 16376.4014 - val_mae: 98.0474 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 15244.5586 - mae: 95.1802 - val_loss: 15972.2256 - val_mae: 97.7225 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 14884.0781 - mae: 95.5068 - val_loss: 15737.1084 - val_mae: 97.8791 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 14648.6582 - mae: 95.4174 - val_loss: 15611.0283 - val_mae: 97.7025 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 14486.0732 - mae: 94.9779 - val_loss: 15534.5078 - val_mae: 97.7817 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 14337.0059 - mae: 94.5816 - val_loss: 15513.9004 - val_mae: 97.7762 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 14217.4453 - mae: 94.0880 - val_loss: 15500.8867 - val_mae: 98.0410 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 14114.6094 - mae: 93.6885 - val_loss: 15498.9922 - val_mae: 98.1717 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 13996.1768 - mae: 93.2586 - val_loss: 15476.7373 - val_mae: 98.1948 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 13918.4297 - mae: 93.0328 - val_loss: 15474.6426 - val_mae: 98.3276 - learning_rate: 0.0010\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step\n",
            "TRANSFORMER - Target 3 - Fold 5 Test RMSE: 122.5187\n",
            "\n",
            "Analyzing feature importance for transformer on Target 3...\n",
            "\n",
            "Visualizing prediction confidence for transformer on Target 3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRANSFORMER - Target 3 - Average CV RMSE: 115.6454\n",
            "\n",
            "Comparing feature importance across targets...\n",
            "\n",
            "Model interpretability analysis completed. Results saved to the 'interpretability' directory.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}